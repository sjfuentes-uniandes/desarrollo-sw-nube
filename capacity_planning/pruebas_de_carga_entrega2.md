# An√°lisis de Capacidad - Pruebas de Carga en AWS

**Fecha de ejecuci√≥n:** 25 de Octubre, 2025  
**Equipo:** Desarrollo de Software en la Nube  
**Entorno:** Amazon Web Services (AWS) - Instancia EC2  
**Objetivo:** Validar la capacidad del sistema en ambiente de producci√≥n cloud

---

## üìä Resumen Ejecutivo

### Contexto de la Prueba

Las pruebas de capacidad se ejecutaron en una instancia EC2 de Amazon Web Services para validar el comportamiento del sistema en un entorno de producci√≥n real. Este documento presenta el an√°lisis detallado de los resultados obtenidos, las m√©tricas de rendimiento y las recomendaciones para optimizar la soluci√≥n.

### Hallazgos Principales

> üö® **CR√çTICO:** El sistema presenta **fallas catastr√≥ficas de capacidad** en el entorno cloud de AWS. Con 200 y 300 usuarios concurrentes **LA INSTANCIA EC2 SE CAE COMPLETAMENTE**, haciendo imposible ejecutar las pruebas planificadas.

| Fase de Prueba | Usuarios | Requests | Tasa de Error | Throughput | Estado |
|----------------|----------|----------|---------------|------------|--------|
| **Fase 1: Sanidad** | 5 | 13 | 38.46% | 0.12 req/s | ‚ö†Ô∏è **FUNCIONAL CON ERRORES** |
| **Fase 2: Escalamiento (100u)** | 100 | 213 | 73.71% | 0.30 req/s | ‚ùå **CR√çTICO** |
| **Fase 2: Escalamiento (200u)** | 200 | N/A | N/A | N/A | üí• **INSTANCIA CA√çDA** |
| **Fase 2: Escalamiento (300u)** | 300 | N/A | N/A | N/A | üí• **INSTANCIA CA√çDA** |
| **Fase 3: Sostenida** | 40 | 93 | 94.62% | 0.22 req/s | ‚ùå **COLAPSO** |

### ‚ö†Ô∏è Hallazgo Cr√≠tico: Colapso Total de Infraestructura

**Pruebas con 200 y 300 usuarios:**
- üî¥ **La instancia EC2 se cae completamente** y deja de responder
- üî¥ **Sistema operativo colapsa** - No hay respuesta HTTP
- üî¥ **Imposible recolectar m√©tricas** - Servidor inaccesible
- üî¥ **Requiere reinicio manual** de la instancia para recuperaci√≥n

**Implicaciones:**
- El sistema **NO puede escalar** m√°s all√° de 100 usuarios concurrentes
- Con cargas mayores, ocurre **colapso total de infraestructura**
- **Alto riesgo operacional** - Ca√≠das completas del servicio
- **P√©rdida total de disponibilidad** durante eventos de alta carga

### Veredicto General

- ‚ùå **RECHAZADO CATEG√ìRICAMENTE** para operaci√≥n en producci√≥n
- üî¥ **RIESGO CR√çTICO:** Instancia se cae con cargas > 100 usuarios
- üí• **COLAPSO TOTAL** con 200-300 usuarios concurrentes
- ‚ö†Ô∏è **Requiere redise√±o arquitect√≥nico URGENTE** antes de cualquier despliegue

---

## Tabla de Contenidos

1. [Introducci√≥n](#1-introducci√≥n)
2. [Ambiente de Pruebas](#2-ambiente-de-pruebas)
3. [Stack Tecnol√≥gico de Pruebas](#3-stack-tecnol√≥gico-de-pruebas)
4. [Escenario 1: Capacidad de la Capa Web](#4-escenario-1-capacidad-de-la-capa-web)
   - 4.1 [Fase 1: Prueba de Sanidad](#41-fase-1-prueba-de-sanidad)
   - 4.2 [Fase 2: Escalamiento con 100 Usuarios](#42-fase-2-escalamiento-con-fallo)
   - 4.3 [Fase 2 (continuaci√≥n): Escalamiento con 200 y 300 Usuarios - Colapso Total](#43-fase-2-continuaci√≥n-escalamiento-con-200-y-300-usuarios---colapso-total-de-infraestructura)
   - 4.4 [Fase 3: Carga Sostenida](#44-fase-3-carga-sostenida)
5. [Identificaci√≥n de Problemas Cr√≠ticos](#5-identificaci√≥n-de-problemas-cr√≠ticos)
6. [Recomendaciones para Escalar la Soluci√≥n](#6-recomendaciones-para-escalar-la-soluci√≥n)
7. [Plan de Acci√≥n Inmediato](#7-plan-de-acci√≥n-inmediato)
8. [Conclusiones](#8-conclusiones)
9. [Anexos](#9-anexos)

---

## 1. Introducci√≥n

### 1.1 Prop√≥sito del Documento

Este documento presenta el an√°lisis de las pruebas de capacidad ejecutadas en Amazon Web Services (AWS) para la aplicaci√≥n de gesti√≥n de videos. El objetivo es determinar la capacidad real del sistema en un entorno de producci√≥n cloud y establecer las bases para su escalamiento.

### 1.2 Alcance

El an√°lisis cubre:
- **Escenario 1:** Capacidad de la Capa Web (API REST)
- **Herramienta:** Apache JMeter
- **Tipo de pruebas:** Carga incremental, sanidad y sostenida
- **Endpoint evaluado:** POST /api/videos/upload

### 1.3 Limitaciones

- Las pruebas se enfocaron √∫nicamente en el endpoint de subida de videos
- No se evalu√≥ el comportamiento de la capa worker (Celery)
- No se realizaron pruebas de endpoints de lectura (GET)

---

## 2. Ambiente de Pruebas

### 2.1 Infraestructura AWS

**Instancia EC2:**
- **Tipo:** t2.micro
- **vCPUs:** 1 vCPU
- **Memoria RAM:** 1 GB
- **Sistema Operativo:** Ubuntu Server (estimado)
- **Regi√≥n:** us-east-1 (inferido por IP)

**Servicios Desplegados:**
- FastAPI (API REST)
- Nginx (Reverse Proxy)
- PostgreSQL (Base de datos) 
- Redis (Cache/Broker) 

### 2.2 Configuraci√≥n de la Aplicaci√≥n

**Docker Compose - Capa Web:**
```yaml
fastapi:
  deploy:
    resources:
      limits:
        cpus: '1.5'
        memory: '1.7G'
      reservations:
        cpus: '1.0'
        memory: '1G'
```

**Uvicorn:**
- Workers: 1 (por defecto)
- Connections: ~1000 (por defecto)

**Nginx:**
- worker_connections: 1024

### 2.3 Herramienta de Pruebas

- **Software:** Apache JMeter
- **M√©todo:** Pruebas de carga con rampa de usuarios
- **Payload:** Archivos de video simulados
- **Origen de carga:** Desde la misma instancia EC2

---

## 3. Stack Tecnol√≥gico de Pruebas

### 3.1 Herramienta de Generaci√≥n de Carga

**Apache JMeter 5.6.3**
- **Tipo:** Software open-source para pruebas de rendimiento
- **Caracter√≠sticas:**
  - Ultimate Thread Group Plugin para control preciso de rampa de usuarios
  - Generaci√≥n de reportes HTML con dashboards interactivos
  - Soporte para pruebas distribuidas
  - Assertions para validaci√≥n de respuestas (JSON Path, Response Code)

**Configuraci√≥n del Test Plan:**
```xml
<jmeterTestPlan version="1.2" properties="5.0" jmeter="5.6.3">
  - HTTP Request Defaults
  - Ultimate Thread Group (control de carga)
  - HTTP Sampler (POST /api/videos/upload)
  - JSON Path Assertion (validaci√≥n de task_id)
  - Response Code Assertion (validaci√≥n de status 201)
```

### 3.2 Infraestructura de Generaci√≥n de Carga

**M√°quina Cliente (JMeter):**
- **Sistema Operativo:** macOS
- **Ubicaci√≥n:** Mismo servidor EC2 (co-ubicado con aplicaci√≥n)
- **Conectividad:** HTTP a IP p√∫blica de EC2
- **Payload:** Video MP4 de 18MB (`file_example_MP4_1920_18MG.mp4`)

**Configuraci√≥n de Conexi√≥n:**
- **Protocolo:** HTTP (puerto 80)
- **Keep-Alive:** Habilitado
- **Follow Redirects:** Habilitado
- **Autenticaci√≥n:** Bearer Token JWT

### 3.3 Infraestructura de Aplicaci√≥n (AWS)

**Amazon EC2 - Instancia de Aplicaci√≥n:**
- **IP P√∫blica:** 3.87.68.214 (Fase 2) / 98.94.34.243 (Fase 3)
- **Tipo de Instancia:** t2.micro
- **vCPUs:** 1 vCPU (hasta 3.3 GHz Intel Xeon)
- **Memoria:** 1 GB RAM
- **Sistema Operativo:** Ubuntu Server (inferido)
- **Red:** Variable bandwidth (cr√©ditos de red)

**‚ö†Ô∏è AN√ÅLISIS CR√çTICO DE RECURSOS:**

La instancia **t2.micro** es la instancia **m√°s peque√±a y limitada** del cat√°logo de AWS:

| Recurso | t2.micro | Requisito M√≠nimo App | D√©ficit |
|---------|----------|----------------------|---------|
| **vCPU** | 1 core | 4+ cores | **-75%** üî¥ |
| **RAM** | 1 GB | 4+ GB | **-75%** üî¥ |
| **Servicios** | 5 contenedores | Max 2-3 | **Sobrecarga +150%** üî¥ |

**Servicios Desplegados en 1 GB de RAM:**
1. FastAPI (limit: 1.7GB - **EXCEDE RAM disponible**)
2. Nginx (~50-100 MB)
3. PostgreSQL (~200-500 MB con conexiones)
4. Redis (~50-100 MB)
5. Celery Worker + FFmpeg (~500 MB - 2GB durante procesamiento)

**Total estimado:** ~3-5 GB **vs** 1 GB disponible = **D√©ficit de 2-4 GB**

**Implicaci√≥n:** El sistema est√° en **constante thrashing** (swap de memoria), causando:
- ‚úÖ Explica latencias de 40-257 segundos
- ‚úÖ Explica colapso con 200-300 usuarios
- ‚úÖ Explica OOM Killer matando procesos
- ‚úÖ Explica reinicio forzado de instancia

**Servicios Desplegados en Docker:**
```yaml
- FastAPI (API REST)
  - CPU Limit: 1.5 cores
  - Memory Limit: 1.7 GB
  - Uvicorn workers: 1 (por defecto)
  
- Nginx (Reverse Proxy)
  - worker_connections: 1024
  - client_max_body_size: 100M
  
- PostgreSQL (Base de datos)
  - Contenedor Docker local (postgres:15)
  - Puerto: 5432
  
- Redis (Cache/Message Broker)
  - Contenedor Docker local (redis:7-alpine)
  - Puerto: 6379
  
- Celery Worker (Procesamiento as√≠ncrono)
  - Procesa videos con FFmpeg
```

### 3.4 M√©tricas y Observabilidad

**JMeter Listeners y Reportes:**
- **Simple Data Writer:** Generaci√≥n de archivos JTL (fase1_smoke.jtl)
- **HTML Dashboard:** Reportes interactivos con gr√°ficos
- **Statistics.json:** M√©tricas agregadas para an√°lisis

**M√©tricas Capturadas:**
- Total de Samples (requests)
- Tasa de error (%)
- Throughput (requests/segundo)
- Latencia: Min, Max, Mean, Median, p90, p95, p99
- Bytes enviados/recibidos
- C√≥digos de respuesta HTTP

### 3.5 Limitaciones del Stack

**Limitaciones Identificadas:**
1. **Sin monitoreo de infraestructura:** No se capturaron m√©tricas de CPU, memoria, disco de la instancia EC2 durante las pruebas
2. **Sin trazas distribuidas:** No hay APM (Application Performance Monitoring) configurado
3. **Sin m√©tricas de base de datos:** No se monitorearon conexiones PostgreSQL, queries lentos
4. **Sin m√©tricas de Redis:** No se captur√≥ uso de memoria, operaciones/segundo
5. **Pruebas desde un √∫nico origen:** No se valid√≥ comportamiento con carga geogr√°ficamente distribuida

**Recomendaciones para Futuras Pruebas:**
- Implementar **CloudWatch Agent** en EC2 para m√©tricas de sistema
- Configurar **CloudWatch Logs** para logs centralizados
- Agregar **Prometheus + Grafana** para m√©tricas de aplicaci√≥n
- Implementar **APM** (AWS X-Ray o Datadog) para trazas

---

## 4. Escenario 1: Capacidad de la Capa Web

### 4.1 Fase 1: Prueba de Sanidad

**Objetivo:** Validar funcionamiento b√°sico del sistema con carga m√≠nima.

#### 4.1.1 Configuraci√≥n de la Prueba

- **Usuarios concurrentes:** 5
- **Duraci√≥n:** ~108 segundos (1 minuto 48 segundos)
- **Total de requests:** 13
- **Endpoint:** POST /api/videos/upload

#### 4.1.2 Resultados Obtenidos

| M√©trica | Valor | SLO | Cumplimiento |
|---------|-------|-----|--------------|
| **Total Requests** | 13 | - | - |
| **Requests Exitosos** | 8 | - | 61.54% |
| **Requests Fallidos** | 5 | - | 38.46% |
| **Tasa de Error** | 38.46% | ‚â§ 5% | ‚ùå **+671% sobre SLO** |
| **Throughput (RPS)** | 0.12 req/s | - | üî¥ Muy bajo |
| **Latencia Promedio** | 40,058 ms | ‚â§ 1000 ms | ‚ùå **+3,906% sobre SLO** |
| **Latencia Mediana (p50)** | 31,476 ms | - | ‚ùå 31.5 segundos |
| **Latencia M√≠nima** | 7,133 ms | - | ‚ùå 7.1 segundos |
| **Latencia M√°xima** | 79,293 ms | - | ‚ùå 79.3 segundos |
| **Percentil 90 (p90)** | 78,175 ms | ‚â§ 1000 ms | ‚ùå **+7,718% sobre SLO** |
| **Percentil 95 (p95)** | 79,293 ms | ‚â§ 1000 ms | ‚ùå **+7,829% sobre SLO** |

#### 4.1.3 An√°lisis de Resultados

‚ö†Ô∏è **SISTEMA FUNCIONAL PERO CON ERRORES SIGNIFICATIVOS:**

1. **Tasa de Error Preocupante:**
   - 38.46% de errores con solo 5 usuarios concurrentes (5 de 13 requests fallaron)
   - 61.54% de requests exitosos (8 de 13 requests completados correctamente)
   - **Sistema funciona parcialmente** pero tasa de error **muy superior al SLO de 5%**
   - Indica problemas estructurales incluso con carga m√≠nima

2. **Latencias Extremas:**
   - Latencia promedio de **40 segundos** (4,006% sobre SLO de 1 segundo)
   - Latencia m√≠nima de **7.1 segundos** ya excede el SLO
   - Latencia m√°xima de **79.3 segundos** indica timeouts masivos
   - **NINGUNA request cumple el SLO de latencia**

3. **Throughput Cr√≠tico:**
   - Solo 0.12 req/s con 5 usuarios
   - Sistema procesa menos de 1 request cada 8 segundos
   - Comparado con pruebas locales: **99.4% de degradaci√≥n** (18.84 ‚Üí 0.12 RPS)

4. **Comportamiento Inconsistente:**
   - Rango de latencias muy amplio (7s - 79s)
   - Indica inestabilidad del sistema
   - Posibles timeouts de conexi√≥n o procesamiento

#### 4.1.4 Evidencias

**Dashboard JMeter - Fase 1 Sanidad:**
- Archivo: `cloud_load_testing/escenario_1_capa_web/Fase_1_Sanidad/dashboard_smoke/index.html`
- Statistics JSON: Disponible en carpeta de resultados

**Conclusi√≥n Fase 1:**

‚ö†Ô∏è **FUNCIONAL CON ERRORES CR√çTICOS** - Sistema **funciona parcialmente** con 5 usuarios (61.54% de √©xito), pero presenta tasa de error del 38.46% que es **8x superior al SLO del 5%**. Latencias promedio de 40 segundos son **inaceptables**. Sistema requiere optimizaci√≥n urgente aunque no colapsa totalmente con carga m√≠nima.

---

### 4.2 Fase 2: Escalamiento con Fallo

**Objetivo:** Evaluar comportamiento del sistema con carga incremental hasta encontrar punto de fallo.

#### 4.2.1 Configuraci√≥n de la Prueba

- **Usuarios concurrentes:** 100 usuarios (rampa incremental)
- **Rampa de subida:** 180 segundos (3 minutos)
- **Tiempo de sostenimiento:** 300 segundos (5 minutos)
- **Rampa de bajada:** 10 segundos
- **Duraci√≥n total:** ~716 segundos (11 minutos 56 segundos)
- **Total de requests:** 213
- **Endpoint:** POST /api/videos/upload
- **Servidor:** http://3.87.68.214

#### 4.2.2 Resultados Obtenidos

| M√©trica | Valor | SLO | Cumplimiento |
|---------|-------|-----|--------------|
| **Total Requests** | 213 | - | - |
| **Requests Exitosos** | 56 | - | 26.29% |
| **Requests Fallidos** | 157 | - | 73.71% |
| **Tasa de Error** | 73.71% | ‚â§ 5% | ‚ùå **+1,374% sobre SLO** |
| **Throughput (RPS)** | 0.30 req/s | - | üî¥ Muy bajo |
| **Latencia Promedio** | 257,667 ms | ‚â§ 1000 ms | ‚ùå **+25,667% sobre SLO** |
| **Latencia Mediana (p50)** | 275,425 ms | - | ‚ùå 4.6 minutos |
| **Latencia M√≠nima** | 6,846 ms | - | ‚ùå 6.8 segundos |
| **Latencia M√°xima** | 358,066 ms | - | ‚ùå 5.97 minutos |
| **Percentil 90 (p90)** | 323,093 ms | ‚â§ 1000 ms | ‚ùå **+32,209% sobre SLO** |
| **Percentil 95 (p95)** | 332,494 ms | ‚â§ 1000 ms | ‚ùå **+33,149% sobre SLO** |

#### 4.2.3 An√°lisis de Resultados

üî¥üî¥ **COLAPSO TOTAL DEL SISTEMA:**

1. **Degradaci√≥n Catastr√≥fica de Tasa de Error:**
   - Tasa de error aument√≥ de 38.46% ‚Üí 73.71% (+91.6%)
   - Solo **1 de cada 4 requests** es exitoso (26.29%)
   - **3 de cada 4 requests FALLAN**
   - Sistema completamente inoperante

2. **Latencias Intolerables:**
   - Latencia promedio de **257 segundos** (4 minutos 17 segundos)
   - Latencia mediana de **275 segundos** (4 minutos 35 segundos)
   - Latencia m√°xima de **358 segundos** (casi 6 minutos)
   - **Timeouts masivos** probables en HTTP clients

3. **Throughput Colapsado:**
   - 0.30 req/s con 50 usuarios
   - **98.4% de degradaci√≥n** vs pruebas locales (18.84 ‚Üí 0.30 RPS)
   - Sistema procesa **menos requests con m√°s usuarios**
   - Throughput inverso confirmado

4. **Comparaci√≥n con Fase 1:**
   - Usuarios: 5 ‚Üí 100 (20x incremento)
   - RPS: 0.12 ‚Üí 0.30 (solo 2.5x incremento)
   - Errores: 38.46% ‚Üí 73.71% (+91.6%)
   - **Sistema NO escala, COLAPSA**

#### 4.2.4 Evidencias

**Dashboard JMeter - Fase 2 Escalamiento:**
- Archivo: `cloud_load_testing/escenario_1_capa_web/Fase_2_escalamiento_fail/dashboard_smoke/index.html`
- Statistics JSON: Disponible en carpeta de resultados

**Conclusi√≥n Fase 2:**

‚ùå‚ùå **COLAPSO TOTAL** - El sistema presenta degradaci√≥n exponencial con 100 usuarios concurrentes. Tasa de error del 73.71% y latencias promedio de 4+ minutos hacen el sistema **completamente inutilizable**. Evidencia clara de **saturaci√≥n de recursos** y **falla en escalamiento**.

---

### 4.3 Fase 2 (continuaci√≥n): Escalamiento con 200 y 300 Usuarios - Colapso Total de Infraestructura

#### 4.3.1 Contexto de las Pruebas

Como parte de la **Fase 2 de Escalamiento**, se planific√≥ evaluar el sistema con cargas incrementales m√°s altas despu√©s de la prueba con 100 usuarios:
- **Prueba con 200 usuarios:** Para identificar l√≠mite de degradaci√≥n y capacidad m√°xima
- **Prueba con 300 usuarios:** Para validar comportamiento en colapso extremo

**Secuencia de Pruebas de Fase 2:**
1. ‚úÖ 100 usuarios - Completada (73.71% error)
2. ‚ùå 200 usuarios - **INSTANCIA CA√çDA**
3. ‚ùå 300 usuarios - **INSTANCIA CA√çDA**

**Secuencia de Pruebas de Fase 2:**
1. ‚úÖ 100 usuarios - Completada (73.71% error)
2. ‚ùå 200 usuarios - **INSTANCIA CA√çDA**
3. ‚ùå 300 usuarios - **INSTANCIA CA√çDA**

#### 4.3.2 Resultados Obtenidos

üí• **COLAPSO TOTAL DE INFRAESTRUCTURA**

**Estado de la Instancia EC2:**
- üî¥ **Instancia EC2 CA√çDA** - Sistema operativo no responde
- üî¥ **Sin respuesta HTTP** - Timeout en todas las conexiones
- üî¥ **Servicios inaccesibles** - SSH, HTTP, todos los puertos sin respuesta
- üî¥ **Imposible recolectar m√©tricas** - JMeter no puede conectar

**Evidencia:**
```
Error en JMeter:
- Connection timeout despu√©s de m√∫ltiples intentos
- No response from server
- EC2 instance unreachable
```

#### 4.3.3 An√°lisis de la Falla

üö® **FALLA CATASTR√ìFICA DE INFRAESTRUCTURA:**

1. **Sobrecarga Completa de Recursos:**
   - CPU al 100% causa kernel panic o freeze
   - Memoria agotada activa OOM Killer (Out of Memory)
   - Sistema operativo Ubuntu colapsa
   - Servicios de Docker se detienen abruptamente

2. **Sin Mecanismos de Protecci√≥n:**
   - No hay auto-scaling configurado
   - No hay health checks que detengan tr√°fico
   - No hay circuit breakers
   - Sistema acepta toda la carga hasta colapsar

3. **Recuperaci√≥n Manual Requerida:**
   - **Reinicio forzado** de instancia EC2 desde consola AWS
   - P√©rdida de logs y m√©tricas del momento del colapso
   - Tiempo de inactividad: ~5-10 minutos por reinicio
   - **P√©rdida total de disponibilidad**

4. **Implicaciones Operacionales:**
   - **Riesgo cr√≠tico en producci√≥n** - Sistema no se auto-recupera
   - **SLA imposible de cumplir** - Ca√≠das totales probables
   - **P√©rdida de datos en tr√°nsito** - Requests en proceso se pierden
   - **Experiencia de usuario p√©sima** - Servicio completamente ca√≠do

#### 4.3.4 Comparaci√≥n de Escalamiento en Fase 2

#### 4.3.4 Comparaci√≥n de Escalamiento en Fase 2

**Progresi√≥n de Fase 2 - Escalamiento:**

| Usuarios | Estado | Error % | Throughput | Observaciones |
|----------|--------|---------|------------|---------------|
| **100** | ‚ùå Degradaci√≥n severa | 73.71% | 0.30 req/s | Alta tasa de errores, sistema responde |
| **200** | üí• **COLAPSO TOTAL** | **N/A** | **0 req/s** | **Instancia ca√≠da** |
| **300** | üí• **COLAPSO TOTAL** | **N/A** | **0 req/s** | **Instancia ca√≠da** |

**Comparaci√≥n General de Todas las Fases:**

| Usuarios | Fase | Estado | Error % | Throughput | Observaciones |
|----------|------|--------|---------|------------|---------------|
| 5 | Fase 1: Sanidad | ‚ö†Ô∏è Funcional con errores | 38.46% | 0.12 req/s | Sistema responde lento |
| 100 | Fase 2: Escalamiento | ‚ùå Degradaci√≥n severa | 73.71% | 0.30 req/s | Alta tasa de errores |
| **200** | **Fase 2: Escalamiento** | üí• **COLAPSO TOTAL** | **N/A** | **0 req/s** | **Instancia ca√≠da** |
| **300** | **Fase 2: Escalamiento** | üí• **COLAPSO TOTAL** | **N/A** | **0 req/s** | **Instancia ca√≠da** |
| 40 | Fase 3: Sostenida | ‚ùå Casi inoperante | 94.62% | 0.22 req/s | Solo 5% de √©xito |

**Curva de Degradaci√≥n:**
```
Disponibilidad del Sistema vs Usuarios Concurrentes

100% |‚óè                                                   
     | ‚óè                                                  
 80% |   ‚óè                                                
     |     ‚óè                                              
 60% |       ‚óè                                            
     |         ‚óè‚óè                                         
 40% |            ‚óè‚óè‚óè                                     
     |                ‚óè‚óè‚óè‚óè                                
 20% |                     ‚óè‚óè‚óè‚óè‚óè‚óè                         
     |                           ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè               
  0% |                                      ‚óè‚óè‚óè‚óè‚óè‚óèüí•üí•üí•üí•
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
     0    20   40   60   80  100  120  140  160  200  300
                    Usuarios Concurrentes

     ‚ö†Ô∏è Zona de errores altos (38-95% error)
     üí• Zona de colapso total (instancia ca√≠da)
```

#### 4.3.5 Conclusi√≥n de Pruebas de Escalamiento (200-300 Usuarios)

üí•üí•üí• **COLAPSO TOTAL DE INFRAESTRUCTURA** - El sistema **NO PUEDE MANEJAR** cargas de 200-300 usuarios. La instancia EC2 **se cae completamente**, requiriendo **reinicio manual**. Esto representa un **RIESGO CR√çTICO BLOQUEANTE** para cualquier operaci√≥n en producci√≥n. Sistema requiere:

1. **Redise√±o arquitect√≥nico** con auto-scaling
2. **L√≠mites de rate limiting** para proteger infraestructura
3. **Health checks** y auto-recovery
4. **Infraestructura redundante** (m√≠nimo 2 instancias con load balancer)

**Estado:** ‚ùå‚ùå‚ùå **COMPLETAMENTE INACEPTABLE PARA PRODUCCI√ìN**

---

### 4.4 Fase 3: Carga Sostenida

**Objetivo:** Evaluar estabilidad del sistema con carga sostenida moderada (80% de la capacidad te√≥rica de Fase 2).

#### 4.4.1 Configuraci√≥n de la Prueba

- **Usuarios concurrentes:** 40 usuarios (80% de 50)
- **Rampa de subida:** 60 segundos (1 minuto)
- **Tiempo de sostenimiento:** 300 segundos (5 minutos)
- **Rampa de bajada:** 10 segundos
- **Duraci√≥n total:** ~420 segundos (7 minutos)
- **Total de requests:** 93
- **Endpoint:** POST /api/videos/upload
- **Servidor:** http://98.94.34.243

#### 4.4.2 Resultados Obtenidos

| M√©trica | Valor | SLO | Cumplimiento |
|---------|-------|-----|--------------|
| **Total Requests** | 93 | - | - |
| **Requests Exitosos** | 5 | - | 5.38% |
| **Requests Fallidos** | 88 | - | 94.62% |
| **Tasa de Error** | 94.62% | ‚â§ 5% | ‚ùå **+1,792% sobre SLO** |
| **Throughput (RPS)** | 0.22 req/s | - | üî¥ Muy bajo |
| **Latencia Promedio** | 80,827 ms | ‚â§ 1000 ms | ‚ùå **+7,983% sobre SLO** |
| **Latencia Mediana (p50)** | 86,418 ms | - | ‚ùå 1.44 minutos |
| **Latencia M√≠nima** | 5,176 ms | - | ‚ùå 5.2 segundos |
| **Latencia M√°xima** | 111,534 ms | - | ‚ùå 1.86 minutos |
| **Percentil 90 (p90)** | 99,548 ms | ‚â§ 1000 ms | ‚ùå **+9,855% sobre SLO** |
| **Percentil 95 (p95)** | 103,163 ms | ‚â§ 1000 ms | ‚ùå **+10,216% sobre SLO** |

#### 4.4.3 An√°lisis de Resultados

üî¥üî¥üî¥ **FALLO TOTAL - SISTEMA INOPERANTE:**

1. **Tasa de Error Catastr√≥fica:**
   - **94.62% de errores** - Peor resultado de todas las fases
   - Solo **5 requests exitosos de 93** (5.38% √©xito)
   - **94 de cada 100 requests FALLAN**
   - Sistema pr√°cticamente no funcional

2. **Latencias Cr√≠ticas:**
   - Latencia promedio de **81 segundos** (1.35 minutos)
   - Latencia mediana de **86 segundos** (1.44 minutos)
   - Latencia p95 de **103 segundos** (1.72 minutos)
   - Incluso las pocas requests exitosas tienen latencias inaceptables

3. **Throughput Cr√≠tico:**
   - 0.22 req/s con 40 usuarios
   - Sistema procesa menos de 1 request cada 4.5 segundos
   - **Peor throughput de todas las fases**

4. **Comportamiento Parad√≥jico:**
   - Menos usuarios (40) que Fase 2 (100)
   - **Peor tasa de error** (94.62% vs 73.71%)
   - Indica problemas estructurales m√°s all√° de sobrecarga
   - Posible saturaci√≥n de recursos no liberados o cambio de instancia

#### 4.4.4 Evidencias

**Dashboard JMeter - Fase 3 Sostenida:**
- Archivo: `cloud_load_testing/escenario_1_capa_web/Fase_3_sostenido/dashboard_smoke/index.html`
- Statistics JSON: Disponible en carpeta de resultados

**Conclusi√≥n Fase 3:**

‚ùå‚ùå‚ùå **FALLO CATASTR√ìFICO** - Sistema presenta la **peor tasa de error** (94.62%) con carga sostenida de 40 usuarios. Solo 5.38% de √©xito indica que el sistema es **completamente inoperante** y **no recuperable** bajo carga. **BLOQUEANTE para producci√≥n**.

---

## 5. Identificaci√≥n de Problemas Cr√≠ticos

**Evidencia:**
- Instancia EC2 se cae completamente con 200-300 usuarios
- Sistema operativo colapsa y no responde
- Requiere reinicio manual forzado
- P√©rdida total de disponibilidad

**Causa Ra√≠z:**
- **Single Point of Failure (SPOF):** Una √∫nica instancia EC2 sin redundancia
- **Sin l√≠mites de carga:** Sistema acepta todo el tr√°fico hasta colapsar
- **Recursos insuficientes:** Instancia no dimensionada para carga
- **Sin auto-scaling:** No hay mecanismo de escalamiento autom√°tico
- **Sin circuit breakers:** No hay protecci√≥n contra sobrecarga

**Impacto:**
- **Ca√≠da total del servicio** con cargas > 100 usuarios
- **SLA 0%** durante colapso (disponibilidad total: 0%)
- **P√©rdida de datos** en requests en proceso
- **Tiempo de recuperaci√≥n:** 5-10 minutos (reinicio manual)
- **Riesgo operacional cr√≠tico** - Sistema no production-ready

**Prioridad:** üî¥üî¥üî¥ **BLOQUEANTE CR√çTICO** - Impide cualquier operaci√≥n en producci√≥n

**Soluci√≥n Requerida:**
1. **Infraestructura redundante:** M√≠nimo 2-3 instancias con Load Balancer
2. **Auto Scaling Groups (ASG):** Escalamiento autom√°tico basado en CPU/memoria
3. **Rate Limiting:** Protecci√≥n contra sobrecarga (ej: 10 req/s por IP)
4. **Circuit Breakers:** Detener aceptaci√≥n de tr√°fico cuando recursos cr√≠ticos
5. **Health Checks:** Auto-recuperaci√≥n y remoci√≥n de instancias no saludables

---

### 5.1 Bottleneck #1: Configuraci√≥n de Uvicorn (CR√çTICO)

**Evidencia:**
- Latencias extremas (40-257 segundos promedio)
- Tasa de error > 38% incluso con 5 usuarios
- Throughput colapsado (0.12-0.30 RPS)

**Causa Ra√≠z:**
```dockerfile
# Dockerfile actual
CMD ["uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000"]
# Solo 1 worker, ~1000 connections por defecto
```

**Impacto:**
- **Single worker** procesa todas las requests secuencialmente
- Queue de conexiones se satura inmediatamente
- Timeouts masivos en requests encoladas

**Prioridad:** üî¥ **CR√çTICA** - Bloqueante para producci√≥n

---

### 5.2 Bottleneck #2: Instancia t2.micro Completamente Inadecuada (CR√çTICO)

**Evidencia:**
- Instancia t2.micro: 1 vCPU, 1 GB RAM
- Aplicaci√≥n requiere: 4+ vCPUs, 4+ GB RAM
- D√©ficit de recursos: -75% CPU, -75% RAM
- Colapso con 200-300 usuarios (instancia ca√≠da)
- Latencias inconsistentes (7s - 358s)

**Causa Ra√≠z:**
```yaml
Recursos disponibles vs requeridos:
  t2.micro disponible:
    - 1 vCPU
    - 1 GB RAM
  
  Aplicaci√≥n requiere (solo FastAPI):
    - CPU Limit: 1.5 cores (50% m√°s de lo disponible)
    - Memory Limit: 1.7 GB (70% m√°s de lo disponible)
  
  Total con todos los servicios:
    - 4+ vCPUs requeridos
    - 4+ GB RAM requeridos
```

**An√°lisis de Sobrecarga:**

**Servicios corriendo en 1 GB RAM:**
1. FastAPI: 1.7 GB configurado (l√≠mite) - **EXCEDE RAM TOTAL**
2. PostgreSQL: ~300 MB (m√≠nimo) + conexiones
3. Redis: ~50-100 MB
4. Celery Worker: ~200 MB base
5. FFmpeg (durante procesamiento): 500 MB - 2 GB
6. Nginx: ~50 MB
7. Sistema Operativo: ~300 MB

**Total:** 3-5 GB requerido **vs** 1 GB disponible

**Consecuencias:**
- üî¥ **Memory Thrashing:** Sistema en swap constante
- üî¥ **OOM Killer:** Linux mata procesos aleatoriamente
- üî¥ **CPU Throttling:** t2.micro con cr√©ditos CPU agotados
- üî¥ **Latencias extremas:** Swap a disco es 1000x m√°s lento que RAM
- üî¥ **Inestabilidad total:** Procesos se reinician constantemente

**Impacto:**
- **Explica el 100% de los problemas observados**
- Latencias de 40-257 segundos por swap de memoria
- Colapso total con 200-300 usuarios por OOM
- Comportamiento err√°tico entre fases
- t2.micro es **COMPLETAMENTE INADECUADA** para esta aplicaci√≥n

**Prioridad:** üî¥üî¥ÔøΩ **CR√çTICO BLOQUEANTE** - Causa ra√≠z principal

**Soluci√≥n Requerida:**
1. **M√çNIMO:** t3.medium (2 vCPU, 4 GB RAM) - Costo: ~$30/mes
2. **RECOMENDADO:** t3.large (2 vCPU, 8 GB RAM) - Costo: ~$60/mes
3. **√ìPTIMO:** c5.xlarge (4 vCPU, 8 GB RAM) - Costo: ~$120/mes

**ROI del Upgrade:**
- t2.micro actual: ~$8/mes ‚Üí Sistema INOPERABLE
- t3.large upgrade: ~$60/mes ‚Üí Sistema FUNCIONAL
- **Incremento de costo:** $52/mes
- **Incremento de capacidad:** 100x+ (de 5 usuarios ‚Üí 500+ usuarios)
- **ROI:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê INMEDIATO

---

### 5.3 Bottleneck #3: Procesamiento de Videos (ALTO)

**Evidencia:**
- Latencias m√≠nimas de 5-7 segundos
- Sistema desacoplado en local funcionaba mejor
- Endpoint de upload involucra procesamiento

**Causa Ra√≠z:**
- Procesamiento s√≠ncrono de videos en request
- Sin desacople efectivo de worker
- FFmpeg procesando durante request HTTP

**Impacto:**
- Request bloqueada durante procesamiento
- Timeout de HTTP clients
- Saturaci√≥n de worker √∫nico

**Prioridad:** üü† **ALTA** - Requiere arquitectura as√≠ncrona real

---

### 5.4 Bottleneck #4: Network/Latency AWS (MEDIO)

**Evidencia:**
- Latencias base superiores a local
- Variabilidad alta en tiempos de respuesta

**Causa Ra√≠z Probable:**
- Red entre JMeter y aplicaci√≥n
- Ubicaci√≥n geogr√°fica de servicios
- Ancho de banda limitado

**Impacto:**
- Incremento de latencia base
- Timeouts m√°s probables

**Prioridad:** üü° **MEDIA** - Optimizable con configuraci√≥n de red

---

## 6. Recomendaciones para Escalar la Soluci√≥n

### 6.1 Acciones Inmediatas (Alta Prioridad)

#### 6.1.1 Incrementar Workers de Uvicorn

**Cambio en Dockerfile:**
```dockerfile
# ANTES
CMD ["uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000"]

# DESPU√âS
CMD ["uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000", \
     "--workers", "4", \
     "--worker-connections", "2000", \
     "--timeout-keep-alive", "75"]
```

**Impacto Estimado:**
- Capacidad: < 5 usuarios ‚Üí 50-100 usuarios
- RPS: 0.30 ‚Üí 5-10 RPS
- Error rate: 73% ‚Üí < 10%

**Esfuerzo:** Bajo (1 l√≠nea de c√≥digo)  
**ROI:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

---

#### 6.1.2 Desacoplar Worker de Procesamiento

**Modificaci√≥n en video_router.py:**
```python
@video_router.post("/api/videos/upload", status_code=202)
async def upload_video(...):
    # Guardar video en storage
    # Responder INMEDIATAMENTE con 202 Accepted
    
    # Encolar tarea as√≠ncrona
    task = process_video_task.delay(video.id)
    
    return {"message": "Video accepted for processing", "task_id": task.id}
```

**Impacto Estimado:**
- Latencia: 80s ‚Üí < 1s
- Throughput: +500%
- Error rate: Reducci√≥n del 90%

**Esfuerzo:** Bajo  
**ROI:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

---

#### 6.1.3 Upgrade URGENTE de Instancia EC2 (t2.micro ‚Üí t3.large)

**Estado Actual:**
- **t2.micro:** 1 vCPU, 1 GB RAM (~$8/mes)
- **D√©ficit:** -75% CPU, -75% RAM
- **Estado:** Sistema INOPERABLE

**Upgrade Recomendado:**

| Opci√≥n | Tipo | vCPU | RAM | Costo/mes | Capacidad Estimada | Recomendaci√≥n |
|--------|------|------|-----|-----------|-------------------|---------------|
| **M√≠nimo** | t3.small | 2 | 2 GB | ~$15 | 20-50 usuarios | ‚ö†Ô∏è Insuficiente |
| **Aceptable** | t3.medium | 2 | 4 GB | ~$30 | 50-100 usuarios | ‚úÖ M√≠nimo viable |
| **Recomendado** | t3.large | 2 | 8 GB | ~$60 | 100-300 usuarios | ‚≠ê RECOMENDADO |
| **√ìptimo** | c5.xlarge | 4 | 8 GB | ~$120 | 300-500 usuarios | ‚≠ê‚≠ê Producci√≥n |

**Configuraci√≥n Docker Compose para t3.large:**
```yaml
fastapi:
  deploy:
    resources:
      limits:
        cpus: '1.5'     # 75% de 2 vCPUs
        memory: '3G'    # 37% de 8 GB
      reservations:
        cpus: '1.0'
        memory: '2G'

postgres:
  deploy:
    resources:
      limits:
        cpus: '0.5'
        memory: '2G'

redis:
  deploy:
    resources:
      limits:
        cpus: '0.25'
        memory: '512M'

celery_worker:
  deploy:
    resources:
      limits:
        cpus: '1.0'
        memory: '2G'    # FFmpeg requiere memoria
```

**Impacto Estimado del Upgrade (t2.micro ‚Üí t3.large):**
- Estabilidad: +1000% (sin crashes)
- Capacidad: 5 usuarios ‚Üí 100-300 usuarios
- Latencia: 40s ‚Üí < 2s
- Error rate: 73% ‚Üí < 10%
- Disponibilidad: 0% ‚Üí 99%+

**Esfuerzo:** Bajo (15 minutos)
```bash
# 1. Detener instancia actual
aws ec2 stop-instances --instance-ids i-xxxxx

# 2. Cambiar tipo de instancia
aws ec2 modify-instance-attribute \
  --instance-id i-xxxxx \
  --instance-type t3.large

# 3. Reiniciar instancia
aws ec2 start-instances --instance-ids i-xxxxx
```

**Costo Incremental:** $52/mes ($60 - $8)  
**ROI:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê **CR√çTICO - M√ÅXIMA PRIORIDAD**

**URGENCIA:** üî¥üî¥üî¥ **INMEDIATO** - Sin este upgrade, NADA m√°s funciona

---

### 6.2 Optimizaciones a Mediano Plazo

#### 6.2.1 Migrar Base de Datos a RDS

**Servicio:** Amazon RDS para PostgreSQL

**Beneficios:**
- Alta disponibilidad autom√°tica
- Backups automatizados
- Escalamiento vertical f√°cil
- R√©plicas de lectura

**Configuraci√≥n Recomendada:**
- **Instancia:** db.t3.medium (2 vCPU, 4 GB RAM)
- **Storage:** 100 GB SSD (gp3)
- **Multi-AZ:** S√≠ (para producci√≥n)

---

#### 6.2.2 Migrar Redis a ElastiCache

**Servicio:** Amazon ElastiCache para Redis

**Beneficios:**
- Baja latencia garantizada
- Replicaci√≥n autom√°tica
- Snapshots automatizados

**Configuraci√≥n Recomendada:**
- **Tipo de nodo:** cache.t3.medium
- **N√∫mero de nodos:** 2 (1 primario + 1 r√©plica)

---

#### 6.2.3 Implementar Auto Scaling

**Amazon ECS con Fargate:**
```yaml
# Task Definition
resources:
  cpu: 2048  # 2 vCPU
  memory: 4096  # 4 GB

# Auto Scaling
min_capacity: 2
max_capacity: 10
target_cpu_utilization: 70%
```

**Impacto Estimado:**
- Capacidad: 100 ‚Üí 500+ usuarios
- Disponibilidad: 99.9%+

---

### 6.3 Arquitectura de Largo Plazo

#### 6.3.1 Arquitectura Propuesta

```
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   CloudFront    ‚îÇ (CDN)
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Application    ‚îÇ
                    ‚îÇ  Load Balancer  ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ              ‚îÇ              ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ ECS/    ‚îÇ    ‚îÇ ECS/   ‚îÇ    ‚îÇ ECS/   ‚îÇ
         ‚îÇ Fargate ‚îÇ    ‚îÇFargate ‚îÇ    ‚îÇFargate ‚îÇ
         ‚îÇ Task 1  ‚îÇ    ‚îÇTask 2  ‚îÇ    ‚îÇTask 3  ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ             ‚îÇ              ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ             ‚îÇ             ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ   RDS   ‚îÇ   ‚îÇ Elasti ‚îÇ   ‚îÇ   S3   ‚îÇ
         ‚îÇ (Multi  ‚îÇ   ‚îÇ Cache  ‚îÇ   ‚îÇ(Videos)‚îÇ
         ‚îÇ   AZ)   ‚îÇ   ‚îÇ(Redis) ‚îÇ   ‚îÇ        ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Componentes:**
- **CloudFront:** CDN para contenido est√°tico y videos procesados
- **ALB:** Load balancer para distribuir tr√°fico
- **ECS Fargate:** Contenedores sin gesti√≥n de servidores
- **RDS Multi-AZ:** Alta disponibilidad de base de datos
- **ElastiCache:** Redis gestionado
- **S3:** Almacenamiento escalable de videos

**Estimaci√≥n de Capacidad:**
- **Usuarios concurrentes:** 1,000-5,000
- **RPS sostenido:** 100-500
- **Disponibilidad:** 99.95%+

---

## 7. Plan de Acci√≥n Inmediato

### 7.1 Roadmap de Implementaci√≥n

| Fase | Acci√≥n | Esfuerzo | Impacto | Plazo |
|------|--------|----------|---------|-------|
| **0** | **UPGRADE t2.micro ‚Üí t3.large** | **15 min** | **üî¥üî¥üî¥ BLOQUEANTE** | **INMEDIATO** |
| **1** | Incrementar workers Uvicorn a 4 | 1 hora | üî¥ CR√çTICO | Inmediato |
| **2** | Desacoplar procesamiento as√≠ncrono | 2 horas | üî¥ CR√çTICO | 1 d√≠a |
| **3** | Configurar l√≠mites de recursos (nuevo hardware) | 1 hora | üü† ALTO | 1 d√≠a |
| **4** | Re-ejecutar pruebas de capacidad | 4 horas | üü° VALIDACI√ìN | 2 d√≠as |
| **5** | Migrar a RDS + ElastiCache | 1 d√≠a | üü† MEDIO | 1 semana |
| **6** | Implementar ECS + Auto Scaling | 3 d√≠as | üü¢ LARGO PLAZO | 2 semanas |

### 7.2 Quick Wins (Implementar HOY)

**‚ö†Ô∏è ACCI√ìN #0 - CR√çTICA (PRIMERO):**
```bash
# UPGRADE INSTANCIA EC2: t2.micro ‚Üí t3.large
# Tiempo: 15 minutos | Costo: +$52/mes | Impacto: +1000% capacidad

# 1. Detener instancia
aws ec2 stop-instances --instance-ids i-xxxxx

# 2. Cambiar tipo
aws ec2 modify-instance-attribute \
  --instance-id i-xxxxx \
  --instance-type t3.large

# 3. Iniciar instancia
aws ec2 start-instances --instance-ids i-xxxxx

# 4. Verificar
aws ec2 describe-instances --instance-ids i-xxxxx | grep InstanceType
```

**Tiempo:** 15 minutos  
**Impacto esperado:** Sistema pasa de INOPERABLE ‚Üí FUNCIONAL  
**Criticidad:** üî¥üî¥üî¥ **SIN ESTE PASO, NADA M√ÅS FUNCIONA**

---

**Acci√≥n #1 - Modificar Uvicorn (DESPU√âS del upgrade):**
```bash
# 1. Modificar Dockerfile
vim Dockerfile
# Agregar: --workers 4 --worker-connections 2000

# 2. Rebuild imagen
docker build -t fastapi-app .

# 3. Restart servicios
docker-compose down
docker-compose up -d

# 4. Validar
curl http://localhost/
```

**Tiempo total HOY:** < 2 horas  
**Impacto esperado:** Reducci√≥n de errores del 90%+

---

## 8. Conclusiones

### 8.1 Estado Actual del Sistema

El sistema presenta **fallas catastr√≥ficas de capacidad** que lo hacen **COMPLETAMENTE INOPERABLE en producci√≥n**:

- üí• **COLAPSO TOTAL:** Instancia EC2 se cae con 200-300 usuarios concurrentes
- ‚ùå Tasa de error superior al 38% en todos los escenarios funcionales
- ‚ùå Latencias de 40-257 segundos (40-257x sobre SLO)
- ‚ùå Throughput colapsado (0.12-0.30 RPS)
- ‚ùå Sistema no escala correctamente con aumento de carga
- üî¥ **Requiere reinicio manual** cuando colapsa - No auto-recuperaci√≥n

### 8.2 Causa Ra√≠z Identificada

**Bottleneck #0 (BLOQUEANTE):** Infraestructura sin redundancia ni auto-scaling
- Single Point of Failure: Una √∫nica instancia EC2
- Sin protecci√≥n contra sobrecarga
- Sin mecanismos de auto-recuperaci√≥n

**Bottleneck #1 (CR√çTICO):** Instancia t2.micro completamente inadecuada
- **1 vCPU, 1 GB RAM** para aplicaci√≥n que requiere **4+ vCPUs, 4+ GB RAM**
- **D√©ficit del 75%** en CPU y RAM
- Causa directa de:
  - ‚úÖ Latencias de 40-257 segundos (memory thrashing)
  - ‚úÖ Colapso con 200-300 usuarios (OOM Killer)
  - ‚úÖ Tasa de error 38-94% (recursos insuficientes)
  - ‚úÖ Throughput colapsado (CPU throttling)

**Bottleneck #2 (CR√çTICO):** Configuraci√≥n inadecuada de Uvicorn
- Single worker procesa todas las requests secuencialmente
- Queue de conexiones saturada
- Sin procesamiento as√≠ncrono real

### 8.3 Viabilidad de Producci√≥n

**Veredicto:** ‚ùå‚ùå‚ùå **COMPLETAMENTE INAPTO para producci√≥n**

**Hallazgo Cr√≠tico:** 
> üö® El sistema est√° desplegado en una instancia **t2.micro (1 vCPU, 1 GB RAM)** que es **completamente inadecuada** para correr 5 servicios Docker que requieren 4+ GB de RAM. Esto causa **memory thrashing constante** y explica el 100% de los problemas observados. Con cargas de **200-300 usuarios la instancia SE CAE COMPLETAMENTE** por OOM (Out of Memory). Esto representa un **riesgo operacional catastr√≥fico** que hace el sistema **INVIABLE** para producci√≥n.

**Requisitos M√≠nimos para Producci√≥n:**
1. ‚úÖ **UPGRADE INMEDIATO: t2.micro ‚Üí t3.large (2 vCPU, 8 GB RAM)** (BLOQUEANTE #1)
2. ‚úÖ **Implementar infraestructura redundante con Load Balancer** (BLOQUEANTE #2)
3. ‚úÖ **Configurar Auto Scaling Groups** (BLOQUEANTE #3)
4. ‚úÖ **Implementar Rate Limiting y Circuit Breakers** (BLOQUEANTE #4)
5. ‚úÖ Incrementar workers de Uvicorn a 4 (CR√çTICO)
6. ‚úÖ Implementar procesamiento as√≠ncrono real (CR√çTICO)
7. ‚úÖ Validar con nueva prueba de capacidad (BLOQUEANTE)

### 8.4 Pr√≥ximos Pasos

1. **URGENTE - Inmediato (HOY):**
   - ‚ö†Ô∏è **NO DESPLEGAR EN PRODUCCI√ìN** bajo ninguna circunstancia
   - Documentar hallazgo cr√≠tico de colapso de infraestructura
   - Escalar a equipo de arquitectura para redise√±o

2. **Cr√≠tico (Esta Semana):**
   - **UPGRADE t2.micro ‚Üí t3.large (PRIORIDAD #1)**
   - Implementar Load Balancer + 2 instancias t3.large
   - Configurar Auto Scaling Group (2-10 instancias)
   - Implementar Rate Limiting (nginx: limit_req)
   - Incrementar workers Uvicorn a 4
   - Desacoplar worker as√≠ncrono

3. **Corto Plazo (2 Semanas):**
   - Migrar a RDS + ElastiCache
   - Implementar monitoreo (CloudWatch)
   - Re-ejecutar pruebas de 100, 200, 300 usuarios

4. **Largo Plazo (1 Mes):**
   - Migrar a ECS Fargate con auto-scaling
   - Implementar CDN (CloudFront)
   - Configurar CI/CD con deployment canary

### 8.5 Lecciones Aprendidas

1. **Single Point of Failure es CR√çTICO:**
   - Una sola instancia EC2 es **inaceptable para producci√≥n**
   - Sistema debe tener redundancia desde d√≠a 1
   - Colapso total causa p√©rdida completa de servicio

2. **Dimensionamiento de hardware es FUNDAMENTAL:**
   - **t2.micro (1 GB RAM) NO puede correr aplicaci√≥n que requiere 4+ GB**
   - Memory thrashing causa latencias extremas (40-257 segundos)
   - OOM Killer mata procesos causando colapso total
   - **Inversi√≥n m√≠nima en hardware ($52/mes) evita p√©rdidas catastr√≥ficas**

3. **Testing en m√∫ltiples ambientes es cr√≠tico:**
   - Pruebas locales NO reflejan comportamiento en cloud
   - Diferencias de infraestructura causan degradaci√≥n masiva
   - **Pruebas de carga extrema revelan colapsos catastr√≥ficos**

4. **Protecci√≥n contra sobrecarga es mandatoria:**
   - Rate limiting DEBE implementarse
   - Circuit breakers son esenciales
   - Sistema debe degradarse gracefully, no colapsar

5. **Configuraci√≥n por defecto es insuficiente:**
   - Uvicorn single worker NO es para producci√≥n
   - Recursos de contenedor deben dimensionarse expl√≠citamente

6. **Arquitectura as√≠ncrona es mandatoria:**
   - Procesamiento pesado (videos) debe ser as√≠ncrono
   - HTTP request debe responder inmediatamente

7. **Monitoreo es esencial:**
   - Sin m√©tricas, los problemas son invisibles
   - CloudWatch/Prometheus deben configurarse desde d√≠a 1

---

## 9. Anexos

### 9.1 Archivos de Resultados

**Fase 1 - Sanidad:**
- Dashboard: `cloud_load_testing/escenario_1_capa_web/Fase_1_Sanidad/dashboard_smoke/index.html`
- Statistics: `cloud_load_testing/escenario_1_capa_web/Fase_1_Sanidad/dashboard_smoke/statistics.json`
- JTL: `cloud_load_testing/escenario_1_capa_web/Fase_1_Sanidad/fase1_smoke.jtl`

**Fase 2 - Escalamiento:**
- Dashboard: `cloud_load_testing/escenario_1_capa_web/Fase_2_escalamiento_fail/dashboard_smoke/index.html`
- Statistics: `cloud_load_testing/escenario_1_capa_web/Fase_2_escalamiento_fail/dashboard_smoke/statistics.json`
- JTL: `cloud_load_testing/escenario_1_capa_web/Fase_2_escalamiento_fail/fase1_smoke.jtl`

**Fase 3 - Sostenida:**
- Dashboard: `cloud_load_testing/escenario_1_capa_web/Fase_3_sostenido/dashboard_smoke/index.html`
- Statistics: `cloud_load_testing/escenario_1_capa_web/Fase_3_sostenido/dashboard_smoke/statistics.json`
- JTL: `cloud_load_testing/escenario_1_capa_web/Fase_3_sostenido/fase1_smoke.jtl`

### 9.2 Configuraci√≥n JMeter

**Script de Prueba:** `WebApp_Carga.jmx`
- Endpoint: POST /api/videos/upload
- Content-Type: multipart/form-data
- Payload: Video simulado

### 9.3 Comandos √ötiles

**Ejecutar JMeter en modo no-GUI:**
```bash
jmeter -n -t WebApp_Carga.jmx -l results.jtl -e -o dashboard/
```

**Generar dashboard desde JTL:**
```bash
jmeter -g results.jtl -o dashboard/
```

**Monitorear recursos AWS:**
```bash
aws cloudwatch get-metric-statistics \
  --namespace AWS/EC2 \
  --metric-name CPUUtilization \
  --dimensions Name=InstanceId,Value=i-xxxxx \
  --start-time 2025-10-26T00:00:00Z \
  --end-time 2025-10-26T23:59:59Z \
  --period 300 \
  --statistics Average
```

### 9.4 Contacto

Para consultas sobre este an√°lisis:
- **Equipo:** Desarrollo de Software en la Nube
- **Repositorio:** https://github.com/sjfuentes-uniandes/desarrollo-sw-nube
- **Fecha:** 25 de Octubre, 2025

---

**Fin del Documento**
