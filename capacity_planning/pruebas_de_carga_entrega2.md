# An√°lisis de Capacidad - Pruebas de Carga en AWS

**Fecha de ejecuci√≥n:** 25 de Octubre, 2025  
**Equipo:** Desarrollo de Software en la Nube  
**Entorno:** Amazon Web Services (AWS) - Instancia EC2  
**Objetivo:** Validar la capacidad del sistema en ambiente de producci√≥n cloud

---

## üìä Resumen Ejecutivo

### Contexto de la Prueba

Las pruebas de capacidad se ejecutaron en una instancia EC2 de Amazon Web Services para validar el comportamiento del sistema en un entorno de producci√≥n real. Este documento presenta el an√°lisis detallado de los resultados obtenidos, las m√©tricas de rendimiento y las recomendaciones para optimizar la soluci√≥n.

### Hallazgos Principales

> üö® **CR√çTICO:** El sistema presenta **fallas catastr√≥ficas de capacidad** en AMBAS capas (Web y Worker) en el entorno cloud de AWS. Con 200 y 300 usuarios concurrentes **LA INSTANCIA EC2 SE CAE COMPLETAMENTE**, y el sistema de workers presenta **91.53% de tasa de error** en procesamiento as√≠ncrono.

**Escenario 1: Capacidad de la Capa Web (API REST)**

| Fase de Prueba | Usuarios | Requests | Tasa de Error | Throughput | Estado |
|----------------|----------|----------|---------------|------------|--------|
| **Fase 1: Sanidad** | 5 | 13 | 38.46% | 0.12 req/s | ‚ö†Ô∏è **FUNCIONAL CON ERRORES** |
| **Fase 2: Escalamiento (100u)** | 100 | 213 | 73.71% | 0.30 req/s | ‚ùå **CR√çTICO** |
| **Fase 2: Escalamiento (200u)** | 200 | N/A | N/A | N/A | üí• **INSTANCIA CA√çDA** |
| **Fase 2: Escalamiento (300u)** | 300 | N/A | N/A | N/A | üí• **INSTANCIA CA√çDA** |
| **Fase 3: Sostenida** | 40 | 93 | 94.62% | 0.22 req/s | ‚ùå **COLAPSO** |

**Escenario 2: Capacidad de la Capa Worker (Procesamiento As√≠ncrono)**

| M√©trica | Valor | Estado |
|---------|-------|--------|
| **Total Tareas Encoladas** | 1,606 | - |
| **Tareas Procesadas Exitosamente** | 136 (8.47%) | ‚ùå **CR√çTICO** |
| **Tareas Fallidas** | 1,470 (91.53%) | üí• **COLAPSO TOTAL** |
| **Throughput de Encolado** | 5.35 tareas/s | üî¥ Muy bajo |
| **Throughput de Procesamiento** | ~0.067 tareas/s (4/min) | üí• **SATURACI√ìN** |
| **Ratio Entrada/Salida** | 80:1 | üí• **BACKLOG INFINITO** |
| **Latencia Promedio** | 1.87 segundos | ‚ö†Ô∏è Timeouts |

### ‚ö†Ô∏è Hallazgos Cr√≠ticos Consolidados

**Escenario 1 - Capa Web (API REST):**
- üî¥ **La instancia EC2 se cae completamente** con 200-300 usuarios
- üî¥ **Sistema operativo colapsa** - No hay respuesta HTTP
- üî¥ **Imposible recolectar m√©tricas** - Servidor inaccesible
- üî¥ **Requiere reinicio manual** de la instancia para recuperaci√≥n
- üî¥ **94.62% de error** con solo 40 usuarios sostenidos

**Escenario 2 - Capa Worker (Procesamiento As√≠ncrono):**
- üî¥ **91.53% de tasa de error** al encolar tareas en Redis
- üî¥ **Ratio 80:1** - Se encolan 80x m√°s r√°pido de lo que se procesan
- üî¥ **Backlog de 1,455+ tareas** acumuladas en solo 5 minutos
- üî¥ **Redis saturado** - Rechaza nuevas conexiones
- üî¥ **Workers completamente saturados** - Solo 4 tareas/minuto vs 321 tareas/minuto entrando

**Implicaciones:**
- El sistema **NO puede escalar** m√°s all√° de 100 usuarios concurrentes (Capa Web)
- Los workers **NUNCA podr√°n alcanzar** la tasa de entrada de tareas (Backlog infinito)
- Con cargas mayores, ocurre **colapso total de infraestructura** en ambas capas
- **Alto riesgo operacional** - Ca√≠das completas del servicio
- **P√©rdida total de disponibilidad** durante eventos de alta carga
- **Videos nunca se procesan** - Experiencia de usuario p√©sima

### Veredicto General

- ‚ùå **RECHAZADO CATEG√ìRICAMENTE** para operaci√≥n en producci√≥n
- üî¥ **RIESGO CR√çTICO:** Instancia se cae con cargas > 100 usuarios (Escenario 1)
- üî¥ **RIESGO CR√çTICO:** Workers con 91.53% de error - Sistema inoperante (Escenario 2)
- üí• **COLAPSO TOTAL** con 200-300 usuarios concurrentes (Ambos Escenarios)
- ‚ö†Ô∏è **Requiere redise√±o arquitect√≥nico URGENTE** antes de cualquier despliegue
- ‚ö†Ô∏è **Separaci√≥n de capas MANDATORIA** - API y Workers deben estar en instancias dedicadas

---

## Tabla de Contenidos

1. [Introducci√≥n](#1-introducci√≥n)
2. [Ambiente de Pruebas](#2-ambiente-de-pruebas)
3. [Stack Tecnol√≥gico de Pruebas](#3-stack-tecnol√≥gico-de-pruebas)
4. [Escenario 1: Capacidad de la Capa Web](#4-escenario-1-capacidad-de-la-capa-web)
   - 4.1 [Fase 1: Prueba de Sanidad](#41-fase-1-prueba-de-sanidad)
   - 4.2 [Fase 2: Escalamiento con 100 Usuarios](#42-fase-2-escalamiento-con-fallo)
   - 4.3 [Fase 2 (continuaci√≥n): Escalamiento con 200 y 300 Usuarios - Colapso Total](#43-fase-2-continuaci√≥n-escalamiento-con-200-y-300-usuarios---colapso-total-de-infraestructura)
   - 4.4 [Fase 3: Carga Sostenida](#44-fase-3-carga-sostenida)
5. [Escenario 2: Capacidad de la Capa Worker](#5-escenario-2-capacidad-de-la-capa-worker)
   - 5.1 [Configuraci√≥n de la Prueba](#51-configuraci√≥n-de-la-prueba)
   - 5.2 [Resultados Obtenidos](#52-resultados-obtenidos)
   - 5.3 [An√°lisis de Causas Ra√≠z](#53-an√°lisis-de-causas-ra√≠z)
   - 5.4 [Comparaci√≥n con Escenario 1](#54-comparaci√≥n-con-escenario-1)
6. [Identificaci√≥n de Problemas Cr√≠ticos (Consolidado)](#6-identificaci√≥n-de-problemas-cr√≠ticos-consolidado)
7. [Recomendaciones para Escalar la Soluci√≥n](#7-recomendaciones-para-escalar-la-soluci√≥n)
8. [Plan de Acci√≥n Inmediato](#8-plan-de-acci√≥n-inmediato)
9. [Conclusiones](#9-conclusiones)
10. [Anexos](#10-anexos)

---

## 1. Introducci√≥n

### 1.1 Prop√≥sito del Documento

Este documento presenta el an√°lisis de las pruebas de capacidad ejecutadas en Amazon Web Services (AWS) para la aplicaci√≥n de gesti√≥n de videos. El objetivo es determinar la capacidad real del sistema en un entorno de producci√≥n cloud y establecer las bases para su escalamiento.

### 1.2 Alcance

El an√°lisis cubre **DOS escenarios** de pruebas de capacidad:

**Escenario 1: Capacidad de la Capa Web**
- **Componente:** API REST (FastAPI)
- **Herramienta:** Apache JMeter (HTTP Samplers)
- **Tipo de pruebas:** Carga incremental, sanidad y sostenida
- **Endpoint evaluado:** POST /api/videos/upload

**Escenario 2: Capacidad de la Capa Worker**
- **Componente:** Celery Workers (procesamiento as√≠ncrono)
- **Herramienta:** Apache JMeter (JSR223 Samplers)
- **Tipo de pruebas:** Inyecci√≥n directa de tareas en cola Redis
- **Cola evaluada:** Redis "celery"

### 1.3 Limitaciones

- Las pruebas se enfocaron en endpoints de escritura (upload)
- No se realizaron pruebas de endpoints de lectura (GET)
- No se evalu√≥ comportamiento con carga geogr√°ficamente distribuida
- No se midi√≥ tiempo end-to-end de procesamiento de video completo

---

## 2. Ambiente de Pruebas

### 2.1 Infraestructura AWS

**Instancia EC2:**
- **Tipo:** t2.small
- **vCPUs:** 2 vCPUs
- **Memoria RAM:** 2 GiB
- **Almacenamiento:** 50 GB
- **Sistema Operativo:** Ubuntu
- **Regi√≥n:** us-east-1 (inferido por IP)

**Servicios Desplegados:**
- FastAPI (API REST)
- Nginx (Reverse Proxy)
- PostgreSQL (Base de datos) 
- Redis (Cache/Broker) 

### 2.2 Configuraci√≥n de la Aplicaci√≥n

**Docker Compose - Capa Web:**
```yaml
fastapi:
  deploy:
    resources:
      limits:
        cpus: '1.5'
        memory: '1.7G'
      reservations:
        cpus: '1.0'
        memory: '1G'
```

**Uvicorn:**
- Workers: 1 (por defecto)
- Connections: ~1000 (por defecto)

**Nginx:**
- worker_connections: 1024

### 2.3 Herramienta de Pruebas

- **Software:** Apache JMeter
- **M√©todo:** Pruebas de carga con rampa de usuarios
- **Payload:** Archivos de video simulados
- **Origen de carga:** Desde la misma instancia EC2

---

## 3. Stack Tecnol√≥gico de Pruebas

### 3.1 Herramienta de Generaci√≥n de Carga

**Apache JMeter 5.6.3**
- **Tipo:** Software open-source para pruebas de rendimiento
- **Caracter√≠sticas:**
  - Ultimate Thread Group Plugin para control preciso de rampa de usuarios
  - Generaci√≥n de reportes HTML con dashboards interactivos
  - Soporte para pruebas distribuidas
  - Assertions para validaci√≥n de respuestas (JSON Path, Response Code)

**Configuraci√≥n del Test Plan:**
```xml
<jmeterTestPlan version="1.2" properties="5.0" jmeter="5.6.3">
  - HTTP Request Defaults
  - Ultimate Thread Group (control de carga)
  - HTTP Sampler (POST /api/videos/upload)
  - JSON Path Assertion (validaci√≥n de task_id)
  - Response Code Assertion (validaci√≥n de status 201)
```

### 3.2 Infraestructura de Generaci√≥n de Carga

**M√°quina Cliente (JMeter):**
- **Sistema Operativo:** macOS
- **Ubicaci√≥n:** Mismo servidor EC2 (co-ubicado con aplicaci√≥n)
- **Conectividad:** HTTP a IP p√∫blica de EC2
- **Payload:** Video MP4 de 18MB (`file_example_MP4_1920_18MG.mp4`)

**Configuraci√≥n de Conexi√≥n:**
- **Protocolo:** HTTP (puerto 80)
- **Keep-Alive:** Habilitado
- **Follow Redirects:** Habilitado
- **Autenticaci√≥n:** Bearer Token JWT

### 3.3 Infraestructura de Aplicaci√≥n (AWS)

**Amazon EC2 - Instancia de Aplicaci√≥n:**
- **IP P√∫blica:** 3.87.68.214 (Fase 2) / 98.94.34.243 (Fase 3)
- **Tipo de Instancia:** t2.small
- **vCPUs:** 2 vCPUs (hasta 3.3 GHz Intel Xeon)
- **Memoria:** 2 GiB RAM
- **Almacenamiento:** 50 GB
- **Sistema Operativo:** Ubuntu
- **Red:** Variable bandwidth (cr√©ditos de red)

**‚ö†Ô∏è AN√ÅLISIS CR√çTICO DE RECURSOS:**

La instancia **t2.small** con 2 vCPUs y 2 GiB RAM sigue siendo **limitada** para la carga de la aplicaci√≥n:

| Recurso | t2.small | Requisito M√≠nimo App | D√©ficit |
|---------|----------|----------------------|---------|
| **vCPU** | 2 cores | 4+ cores | **-50%** üî¥ |
| **RAM** | 2 GiB | 4+ GiB | **-50%** üî¥ |
| **Servicios** | 5 contenedores | Max 3-4 | **Sobrecarga +25%** ÔøΩ |

**Servicios Desplegados en 2 GiB de RAM:**
1. FastAPI (limit: 1.7GB - **AJUSTADO pero limitado**)
2. Nginx (~50-100 MB)
3. PostgreSQL (~200-500 MB con conexiones)
4. Redis (~50-100 MB)
5. Celery Worker + FFmpeg (~500 MB - 2GB durante procesamiento)

**Total estimado:** ~3-5 GiB **vs** 2 GiB disponible = **D√©ficit de 1-3 GiB**

**Implicaci√≥n:** El sistema puede experimentar **presi√≥n de memoria** (swap), causando:
- ‚úÖ Explica latencias de 40-257 segundos
- ‚úÖ Explica colapso con 200-300 usuarios
- ‚úÖ Explica posibles OOM Killer matando procesos
- ‚úÖ Explica degradaci√≥n bajo carga

**Servicios Desplegados en Docker:**
```yaml
- FastAPI (API REST)
  - CPU Limit: 1.5 cores
  - Memory Limit: 1.7 GB
  - Uvicorn workers: 1 (por defecto)
  
- Nginx (Reverse Proxy)
  - worker_connections: 1024
  - client_max_body_size: 100M
  
- PostgreSQL (Base de datos)
  - Contenedor Docker local (postgres:15)
  - Puerto: 5432
  
- Redis (Cache/Message Broker)
  - Contenedor Docker local (redis:7-alpine)
  - Puerto: 6379
  
- Celery Worker (Procesamiento as√≠ncrono)
  - Procesa videos con FFmpeg
```

### 3.4 M√©tricas y Observabilidad

**JMeter Listeners y Reportes:**
- **Simple Data Writer:** Generaci√≥n de archivos JTL (fase1_smoke.jtl)
- **HTML Dashboard:** Reportes interactivos con gr√°ficos
- **Statistics.json:** M√©tricas agregadas para an√°lisis

**M√©tricas Capturadas:**
- Total de Samples (requests)
- Tasa de error (%)
- Throughput (requests/segundo)
- Latencia: Min, Max, Mean, Median, p90, p95, p99
- Bytes enviados/recibidos
- C√≥digos de respuesta HTTP

### 3.5 Limitaciones del Stack

**Limitaciones Identificadas:**
1. **Sin monitoreo de infraestructura:** No se capturaron m√©tricas de CPU, memoria, disco de la instancia EC2 durante las pruebas
2. **Sin trazas distribuidas:** No hay APM (Application Performance Monitoring) configurado
3. **Sin m√©tricas de base de datos:** No se monitorearon conexiones PostgreSQL, queries lentos
4. **Sin m√©tricas de Redis:** No se captur√≥ uso de memoria, operaciones/segundo
5. **Pruebas desde un √∫nico origen:** No se valid√≥ comportamiento con carga geogr√°ficamente distribuida

**Recomendaciones para Futuras Pruebas:**
- Implementar **CloudWatch Agent** en EC2 para m√©tricas de sistema
- Configurar **CloudWatch Logs** para logs centralizados
- Agregar **Prometheus + Grafana** para m√©tricas de aplicaci√≥n
- Implementar **APM** (AWS X-Ray o Datadog) para trazas

---

## 4. Escenario 1: Capacidad de la Capa Web

### 4.1 Fase 1: Prueba de Sanidad

**Objetivo:** Validar funcionamiento b√°sico del sistema con carga m√≠nima.

#### 4.1.1 Configuraci√≥n de la Prueba

- **Usuarios concurrentes:** 5
- **Duraci√≥n:** ~108 segundos (1 minuto 48 segundos)
- **Total de requests:** 13
- **Endpoint:** POST /api/videos/upload

#### 4.1.2 Resultados Obtenidos

| M√©trica | Valor | SLO | Cumplimiento |
|---------|-------|-----|--------------|
| **Total Requests** | 13 | - | - |
| **Requests Exitosos** | 8 | - | 61.54% |
| **Requests Fallidos** | 5 | - | 38.46% |
| **Tasa de Error** | 38.46% | ‚â§ 5% | ‚ùå **+671% sobre SLO** |
| **Throughput (RPS)** | 0.12 req/s | - | üî¥ Muy bajo |
| **Latencia Promedio** | 40,058 ms | ‚â§ 1000 ms | ‚ùå **+3,906% sobre SLO** |
| **Latencia Mediana (p50)** | 31,476 ms | - | ‚ùå 31.5 segundos |
| **Latencia M√≠nima** | 7,133 ms | - | ‚ùå 7.1 segundos |
| **Latencia M√°xima** | 79,293 ms | - | ‚ùå 79.3 segundos |
| **Percentil 90 (p90)** | 78,175 ms | ‚â§ 1000 ms | ‚ùå **+7,718% sobre SLO** |
| **Percentil 95 (p95)** | 79,293 ms | ‚â§ 1000 ms | ‚ùå **+7,829% sobre SLO** |

#### 4.1.3 An√°lisis de Resultados

‚ö†Ô∏è **SISTEMA FUNCIONAL PERO CON ERRORES SIGNIFICATIVOS:**

1. **Tasa de Error Preocupante:**
   - 38.46% de errores con solo 5 usuarios concurrentes (5 de 13 requests fallaron)
   - 61.54% de requests exitosos (8 de 13 requests completados correctamente)
   - **Sistema funciona parcialmente** pero tasa de error **muy superior al SLO de 5%**
   - Indica problemas estructurales incluso con carga m√≠nima

2. **Latencias Extremas:**
   - Latencia promedio de **40 segundos** (4,006% sobre SLO de 1 segundo)
   - Latencia m√≠nima de **7.1 segundos** ya excede el SLO
   - Latencia m√°xima de **79.3 segundos** indica timeouts masivos
   - **NINGUNA request cumple el SLO de latencia**

3. **Throughput Cr√≠tico:**
   - Solo 0.12 req/s con 5 usuarios
   - Sistema procesa menos de 1 request cada 8 segundos
   - Comparado con pruebas locales: **99.4% de degradaci√≥n** (18.84 ‚Üí 0.12 RPS)

4. **Comportamiento Inconsistente:**
   - Rango de latencias muy amplio (7s - 79s)
   - Indica inestabilidad del sistema
   - Posibles timeouts de conexi√≥n o procesamiento

#### 4.1.4 Evidencias

**Dashboard JMeter - Fase 1 Sanidad:**
- Archivo: `cloud_load_testing/escenario_1_capa_web/Fase_1_Sanidad/dashboard_smoke/index.html`
- Statistics JSON: Disponible en carpeta de resultados

**Conclusi√≥n Fase 1:**

‚ö†Ô∏è **FUNCIONAL CON ERRORES CR√çTICOS** - Sistema **funciona parcialmente** con 5 usuarios (61.54% de √©xito), pero presenta tasa de error del 38.46% que es **8x superior al SLO del 5%**. Latencias promedio de 40 segundos son **inaceptables**. Sistema requiere optimizaci√≥n urgente aunque no colapsa totalmente con carga m√≠nima.

---

### 4.2 Fase 2: Escalamiento con Fallo

**Objetivo:** Evaluar comportamiento del sistema con carga incremental hasta encontrar punto de fallo.

#### 4.2.1 Configuraci√≥n de la Prueba

- **Usuarios concurrentes:** 100 usuarios (rampa incremental)
- **Rampa de subida:** 180 segundos (3 minutos)
- **Tiempo de sostenimiento:** 300 segundos (5 minutos)
- **Rampa de bajada:** 10 segundos
- **Duraci√≥n total:** ~716 segundos (11 minutos 56 segundos)
- **Total de requests:** 213
- **Endpoint:** POST /api/videos/upload
- **Servidor:** http://3.87.68.214

#### 4.2.2 Resultados Obtenidos

| M√©trica | Valor | SLO | Cumplimiento |
|---------|-------|-----|--------------|
| **Total Requests** | 213 | - | - |
| **Requests Exitosos** | 56 | - | 26.29% |
| **Requests Fallidos** | 157 | - | 73.71% |
| **Tasa de Error** | 73.71% | ‚â§ 5% | ‚ùå **+1,374% sobre SLO** |
| **Throughput (RPS)** | 0.30 req/s | - | üî¥ Muy bajo |
| **Latencia Promedio** | 257,667 ms | ‚â§ 1000 ms | ‚ùå **+25,667% sobre SLO** |
| **Latencia Mediana (p50)** | 275,425 ms | - | ‚ùå 4.6 minutos |
| **Latencia M√≠nima** | 6,846 ms | - | ‚ùå 6.8 segundos |
| **Latencia M√°xima** | 358,066 ms | - | ‚ùå 5.97 minutos |
| **Percentil 90 (p90)** | 323,093 ms | ‚â§ 1000 ms | ‚ùå **+32,209% sobre SLO** |
| **Percentil 95 (p95)** | 332,494 ms | ‚â§ 1000 ms | ‚ùå **+33,149% sobre SLO** |

#### 4.2.3 An√°lisis de Resultados

üî¥üî¥ **COLAPSO TOTAL DEL SISTEMA:**

1. **Degradaci√≥n Catastr√≥fica de Tasa de Error:**
   - Tasa de error aument√≥ de 38.46% ‚Üí 73.71% (+91.6%)
   - Solo **1 de cada 4 requests** es exitoso (26.29%)
   - **3 de cada 4 requests FALLAN**
   - Sistema completamente inoperante

2. **Latencias Intolerables:**
   - Latencia promedio de **257 segundos** (4 minutos 17 segundos)
   - Latencia mediana de **275 segundos** (4 minutos 35 segundos)
   - Latencia m√°xima de **358 segundos** (casi 6 minutos)
   - **Timeouts masivos** probables en HTTP clients

3. **Throughput Colapsado:**
   - 0.30 req/s con 50 usuarios
   - **98.4% de degradaci√≥n** vs pruebas locales (18.84 ‚Üí 0.30 RPS)
   - Sistema procesa **menos requests con m√°s usuarios**
   - Throughput inverso confirmado

4. **Comparaci√≥n con Fase 1:**
   - Usuarios: 5 ‚Üí 100 (20x incremento)
   - RPS: 0.12 ‚Üí 0.30 (solo 2.5x incremento)
   - Errores: 38.46% ‚Üí 73.71% (+91.6%)
   - **Sistema NO escala, COLAPSA**

#### 4.2.4 Evidencias

**Dashboard JMeter - Fase 2 Escalamiento:**
- Archivo: `cloud_load_testing/escenario_1_capa_web/Fase_2_escalamiento_fail/dashboard_smoke/index.html`
- Statistics JSON: Disponible en carpeta de resultados

**Conclusi√≥n Fase 2:**

‚ùå‚ùå **COLAPSO TOTAL** - El sistema presenta degradaci√≥n exponencial con 100 usuarios concurrentes. Tasa de error del 73.71% y latencias promedio de 4+ minutos hacen el sistema **completamente inutilizable**. Evidencia clara de **saturaci√≥n de recursos** y **falla en escalamiento**.

---

### 4.3 Fase 2 (continuaci√≥n): Escalamiento con 200 y 300 Usuarios - Colapso Total de Infraestructura

#### 4.3.1 Contexto de las Pruebas

Como parte de la **Fase 2 de Escalamiento**, se planific√≥ evaluar el sistema con cargas incrementales m√°s altas despu√©s de la prueba con 100 usuarios:
- **Prueba con 200 usuarios:** Para identificar l√≠mite de degradaci√≥n y capacidad m√°xima
- **Prueba con 300 usuarios:** Para validar comportamiento en colapso extremo

**Secuencia de Pruebas de Fase 2:**
1. ‚úÖ 100 usuarios - Completada (73.71% error)
2. ‚ùå 200 usuarios - **INSTANCIA CA√çDA**
3. ‚ùå 300 usuarios - **INSTANCIA CA√çDA**

**Secuencia de Pruebas de Fase 2:**
1. ‚úÖ 100 usuarios - Completada (73.71% error)
2. ‚ùå 200 usuarios - **INSTANCIA CA√çDA**
3. ‚ùå 300 usuarios - **INSTANCIA CA√çDA**

#### 4.3.2 Resultados Obtenidos

üí• **COLAPSO TOTAL DE INFRAESTRUCTURA**

**Estado de la Instancia EC2:**
- üî¥ **Instancia EC2 CA√çDA** - Sistema operativo no responde
- üî¥ **Sin respuesta HTTP** - Timeout en todas las conexiones
- üî¥ **Servicios inaccesibles** - SSH, HTTP, todos los puertos sin respuesta
- üî¥ **Imposible recolectar m√©tricas** - JMeter no puede conectar

**Evidencia:**
```
Error en JMeter:
- Connection timeout despu√©s de m√∫ltiples intentos
- No response from server
- EC2 instance unreachable
```

#### 4.3.3 An√°lisis de la Falla

üö® **FALLA CATASTR√ìFICA DE INFRAESTRUCTURA:**

1. **Sobrecarga Completa de Recursos:**
   - CPU al 100% causa kernel panic o freeze
   - Memoria agotada activa OOM Killer (Out of Memory)
   - Sistema operativo Ubuntu colapsa
   - Servicios de Docker se detienen abruptamente

2. **Sin Mecanismos de Protecci√≥n:**
   - No hay auto-scaling configurado
   - No hay health checks que detengan tr√°fico
   - No hay circuit breakers
   - Sistema acepta toda la carga hasta colapsar

3. **Recuperaci√≥n Manual Requerida:**
   - **Reinicio forzado** de instancia EC2 desde consola AWS
   - P√©rdida de logs y m√©tricas del momento del colapso
   - Tiempo de inactividad: ~5-10 minutos por reinicio
   - **P√©rdida total de disponibilidad**

4. **Implicaciones Operacionales:**
   - **Riesgo cr√≠tico en producci√≥n** - Sistema no se auto-recupera
   - **SLA imposible de cumplir** - Ca√≠das totales probables
   - **P√©rdida de datos en tr√°nsito** - Requests en proceso se pierden
   - **Experiencia de usuario p√©sima** - Servicio completamente ca√≠do

#### 4.3.4 Comparaci√≥n de Escalamiento en Fase 2

#### 4.3.4 Comparaci√≥n de Escalamiento en Fase 2

**Progresi√≥n de Fase 2 - Escalamiento:**

| Usuarios | Estado | Error % | Throughput | Observaciones |
|----------|--------|---------|------------|---------------|
| **100** | ‚ùå Degradaci√≥n severa | 73.71% | 0.30 req/s | Alta tasa de errores, sistema responde |
| **200** | üí• **COLAPSO TOTAL** | **N/A** | **0 req/s** | **Instancia ca√≠da** |
| **300** | üí• **COLAPSO TOTAL** | **N/A** | **0 req/s** | **Instancia ca√≠da** |

**Comparaci√≥n General de Todas las Fases:**

| Usuarios | Fase | Estado | Error % | Throughput | Observaciones |
|----------|------|--------|---------|------------|---------------|
| 5 | Fase 1: Sanidad | ‚ö†Ô∏è Funcional con errores | 38.46% | 0.12 req/s | Sistema responde lento |
| 100 | Fase 2: Escalamiento | ‚ùå Degradaci√≥n severa | 73.71% | 0.30 req/s | Alta tasa de errores |
| **200** | **Fase 2: Escalamiento** | üí• **COLAPSO TOTAL** | **N/A** | **0 req/s** | **Instancia ca√≠da** |
| **300** | **Fase 2: Escalamiento** | üí• **COLAPSO TOTAL** | **N/A** | **0 req/s** | **Instancia ca√≠da** |
| 40 | Fase 3: Sostenida | ‚ùå Casi inoperante | 94.62% | 0.22 req/s | Solo 5% de √©xito |

**Curva de Degradaci√≥n:**
```
Disponibilidad del Sistema vs Usuarios Concurrentes

100% |‚óè                                                   
     | ‚óè                                                  
 80% |   ‚óè                                                
     |     ‚óè                                              
 60% |       ‚óè                                            
     |         ‚óè‚óè                                         
 40% |            ‚óè‚óè‚óè                                     
     |                ‚óè‚óè‚óè‚óè                                
 20% |                     ‚óè‚óè‚óè‚óè‚óè‚óè                         
     |                           ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè               
  0% |                                      ‚óè‚óè‚óè‚óè‚óè‚óèüí•üí•üí•üí•
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
     0    20   40   60   80  100  120  140  160  200  300
                    Usuarios Concurrentes

     ‚ö†Ô∏è Zona de errores altos (38-95% error)
     üí• Zona de colapso total (instancia ca√≠da)
```

#### 4.3.5 Conclusi√≥n de Pruebas de Escalamiento (200-300 Usuarios)

üí•üí•üí• **COLAPSO TOTAL DE INFRAESTRUCTURA** - El sistema **NO PUEDE MANEJAR** cargas de 200-300 usuarios. La instancia EC2 **se cae completamente**, requiriendo **reinicio manual**. Esto representa un **RIESGO CR√çTICO BLOQUEANTE** para cualquier operaci√≥n en producci√≥n. Sistema requiere:

1. **Redise√±o arquitect√≥nico** con auto-scaling
2. **L√≠mites de rate limiting** para proteger infraestructura
3. **Health checks** y auto-recovery
4. **Infraestructura redundante** (m√≠nimo 2 instancias con load balancer)

**Estado:** ‚ùå‚ùå‚ùå **COMPLETAMENTE INACEPTABLE PARA PRODUCCI√ìN**

---

### 4.4 Fase 3: Carga Sostenida

**Objetivo:** Evaluar estabilidad del sistema con carga sostenida moderada (80% de la capacidad te√≥rica de Fase 2).

#### 4.4.1 Configuraci√≥n de la Prueba

- **Usuarios concurrentes:** 40 usuarios (80% de 50)
- **Rampa de subida:** 60 segundos (1 minuto)
- **Tiempo de sostenimiento:** 300 segundos (5 minutos)
- **Rampa de bajada:** 10 segundos
- **Duraci√≥n total:** ~420 segundos (7 minutos)
- **Total de requests:** 93
- **Endpoint:** POST /api/videos/upload
- **Servidor:** http://98.94.34.243

#### 4.4.2 Resultados Obtenidos

| M√©trica | Valor | SLO | Cumplimiento |
|---------|-------|-----|--------------|
| **Total Requests** | 93 | - | - |
| **Requests Exitosos** | 5 | - | 5.38% |
| **Requests Fallidos** | 88 | - | 94.62% |
| **Tasa de Error** | 94.62% | ‚â§ 5% | ‚ùå **+1,792% sobre SLO** |
| **Throughput (RPS)** | 0.22 req/s | - | üî¥ Muy bajo |
| **Latencia Promedio** | 80,827 ms | ‚â§ 1000 ms | ‚ùå **+7,983% sobre SLO** |
| **Latencia Mediana (p50)** | 86,418 ms | - | ‚ùå 1.44 minutos |
| **Latencia M√≠nima** | 5,176 ms | - | ‚ùå 5.2 segundos |
| **Latencia M√°xima** | 111,534 ms | - | ‚ùå 1.86 minutos |
| **Percentil 90 (p90)** | 99,548 ms | ‚â§ 1000 ms | ‚ùå **+9,855% sobre SLO** |
| **Percentil 95 (p95)** | 103,163 ms | ‚â§ 1000 ms | ‚ùå **+10,216% sobre SLO** |

#### 4.4.3 An√°lisis de Resultados

üî¥üî¥üî¥ **FALLO TOTAL - SISTEMA INOPERANTE:**

1. **Tasa de Error Catastr√≥fica:**
   - **94.62% de errores** - Peor resultado de todas las fases
   - Solo **5 requests exitosos de 93** (5.38% √©xito)
   - **94 de cada 100 requests FALLAN**
   - Sistema pr√°cticamente no funcional

2. **Latencias Cr√≠ticas:**
   - Latencia promedio de **81 segundos** (1.35 minutos)
   - Latencia mediana de **86 segundos** (1.44 minutos)
   - Latencia p95 de **103 segundos** (1.72 minutos)
   - Incluso las pocas requests exitosas tienen latencias inaceptables

3. **Throughput Cr√≠tico:**
   - 0.22 req/s con 40 usuarios
   - Sistema procesa menos de 1 request cada 4.5 segundos
   - **Peor throughput de todas las fases**

4. **Comportamiento Parad√≥jico:**
   - Menos usuarios (40) que Fase 2 (100)
   - **Peor tasa de error** (94.62% vs 73.71%)
   - Indica problemas estructurales m√°s all√° de sobrecarga
   - Posible saturaci√≥n de recursos no liberados o cambio de instancia

#### 4.4.4 Evidencias

**Dashboard JMeter - Fase 3 Sostenida:**
- Archivo: `cloud_load_testing/escenario_1_capa_web/Fase_3_sostenido/dashboard_smoke/index.html`
- Statistics JSON: Disponible en carpeta de resultados

**Conclusi√≥n Fase 3:**

‚ùå‚ùå‚ùå **FALLO CATASTR√ìFICO** - Sistema presenta la **peor tasa de error** (94.62%) con carga sostenida de 40 usuarios. Solo 5.38% de √©xito indica que el sistema es **completamente inoperante** y **no recuperable** bajo carga. **BLOQUEANTE para producci√≥n**.

---

## 5. Escenario 2: Capacidad de la Capa Worker

### 5.1 Configuraci√≥n de la Prueba

**Objetivo:** Medir la capacidad de los Celery Workers para procesar tareas de la cola Redis bajo carga sostenida.

Este escenario eval√∫a la capacidad de la **capa de procesamiento as√≠ncrono** (Celery Workers) que consume tareas de la cola Redis y procesa videos con FFmpeg. A diferencia del Escenario 1 que evalu√≥ la API REST, este escenario mide el **throughput de procesamiento de la cola de tareas** simulando carga directa en Redis mediante scripts JSR223 de JMeter.

**Par√°metros de Prueba:**
- **Threads (productores):** 10 threads concurrentes
- **Ramp-up:** 1 segundo (subida inmediata)
- **Duraci√≥n:** 300 segundos (5 minutos)
- **Loop:** Infinito durante duraci√≥n
- **Payload:** Mensaje de 10MB por tarea
- **Cola Redis:** "celery"
- **Herramienta:** Apache JMeter con JSR223 Sampler (Groovy + Jedis)

**Diferencias con Escenario 1:**

| Aspecto | Escenario 1 (Capa Web) | Escenario 2 (Capa Worker) |
|---------|------------------------|---------------------------|
| **Componente** | API REST (FastAPI) | Celery Workers |
| **Protocolo** | HTTP POST | Redis LPUSH |
| **Endpoint** | POST /api/videos/upload | Cola Redis "celery" |
| **Medici√≥n** | Latencia HTTP, throughput API | Throughput de encolado, capacidad de procesamiento |
| **Carga** | Usuarios concurrentes | Threads encolando tareas |

**Script JSR223 (Groovy) Utilizado:**
```groovy
import redis.clients.jedis.Jedis
import java.nio.file.Files
import java.nio.file.Paths

// Configuraci√≥n
String host = vars.get("REDIS_HOST")      // localhost
int port = vars.get("REDIS_PORT").toInteger()  // 6379
String queue = vars.get("REDIS_QUEUE")    // celery
String payloadPath = vars.get("PAYLOAD_FILE")  // /path/to/mensaje_10mb.txt

Jedis jedis = null

try {
    // 1. Leer payload de 10MB
    String payload = new String(Files.readAllBytes(Paths.get(payloadPath)))

    // 2. Conectar a Redis
    jedis = new Jedis(host, port)
    
    // 3. Ejecutar LPUSH a cola Celery
    jedis.lpush(queue, payload)

    // 4. Reportar √©xito
    SampleResult.setSuccessful(true)
    SampleResult.setResponseCodeOK()
    SampleResult.setResponseMessage("OK - LPUSHed to " + queue)

} catch (Exception e) {
    // 5. Reportar falla
    SampleResult.setSuccessful(false)
    SampleResult.setResponseCode("500")
    SampleResult.setResponseMessage("Error: " + e.getMessage())
} finally {
    // 6. Cerrar conexi√≥n
    if (jedis != null) {
        jedis.close()
    }
}
```

**Configuraci√≥n Celery Worker:**
```yaml
worker:
  deploy:
    resources:
      limits:
        cpus: '1.5'        # 50% M√ÅS de lo disponible
        memory: '1.7G'     # 70% M√ÅS de lo disponible
  command: celery -A src.core.celery_app worker --loglevel=info --concurrency=4
```

**C√°lculo de Carga Esperada:**
- **10 threads** encolando constantemente durante **300 segundos**
- **Tasa esperada:** ~10-50 tareas/segundo (depende de latencia de encolado)
- **Total esperado:** 3,000 - 15,000 tareas encoladas

---

### 5.2 Resultados Obtenidos

> üö® **CR√çTICO:** El sistema de workers presenta **falla catastr√≥fica** con una tasa de error del **91.53%**. Solo **8.47% de las tareas se procesan exitosamente**, indicando **colapso total del sistema de procesamiento as√≠ncrono**.

#### 5.2.1 M√©tricas Generales

| M√©trica | Valor | Estado |
|---------|-------|--------|
| **Total de Tareas Encoladas** | 1,606 | Menor a lo esperado (3K-15K) |
| **Tareas Exitosas (encolado)** | 136 | Solo 8.47% ‚úÖ |
| **Tareas Fallidas (encolado)** | 1,470 | 91.53% ‚ùå |
| **Tasa de Error** | 91.53% | üî¥ **CR√çTICO** |
| **Throughput (tareas/s)** | 5.35 tareas/s | üî¥ Muy bajo |
| **Latencia Promedio** | 1,865 ms | ‚ö†Ô∏è 1.87 segundos |
| **Latencia Mediana (p50)** | 2,005 ms | ‚ö†Ô∏è 2.01 segundos |
| **Latencia M√≠nima** | 279 ms | ‚úÖ 0.28 segundos |
| **Latencia M√°xima** | 2,027 ms | ‚ö†Ô∏è 2.03 segundos |
| **Percentil 90 (p90)** | 2,008 ms | ‚ö†Ô∏è 2.01 segundos |
| **Percentil 95 (p95)** | 2,010 ms | ‚ö†Ô∏è 2.01 segundos |
| **Percentil 99 (p99)** | 2,012 ms | ‚ö†Ô∏è 2.01 segundos |

**Tasa Real Obtenida vs Esperada:**
- **Total encolado:** 1,606 tareas (vs esperado: 3,000-15,000)
- **Duraci√≥n real:** ~300 segundos
- **Throughput de encolado:** 5.35 tareas/segundo (vs esperado: 10-50 tareas/s)

#### 5.2.2 An√°lisis Detallado de Resultados

**1. Tasa de Error Catastr√≥fica: 91.53%**

- **Solo 136 operaciones LPUSH exitosas de 1,606**
- **1,470 fallos** al intentar encolar tareas en Redis
- Indica que el **problema NO es en el procesamiento**, sino en el **encolado**

**Posibles Causas:**
- ‚úÖ **Redis saturado** - No acepta m√°s conexiones
- ‚úÖ **Jedis (cliente) falla** - Timeouts de conexi√≥n
- ‚úÖ **Red saturada** - Ancho de banda limitado de t2.small
- ‚úÖ **Memoria de Redis agotada** - No puede almacenar m√°s mensajes
- ‚úÖ **CPU de EC2 al 100%** - Redis no puede responder a tiempo

**2. Throughput Colapsado: 5.35 ops/s**

- **Expectativa:** 10-50 ops/s con 10 threads
- **Realidad:** 5.35 ops/s
- **Degradaci√≥n:** -50% a -90% vs esperado

**Comparaci√≥n con capacidad te√≥rica:**
```
Throughput te√≥rico con 10 threads:
- Si cada LPUSH toma 100ms ‚Üí 10 ops/s por thread ‚Üí 100 ops/s total
- Realidad: 5.35 ops/s ‚Üí Solo el 5% de capacidad te√≥rica
```

**3. Latencias Concentradas en ~2 segundos**

**Distribuci√≥n de Latencias:**
- **M√≠nima:** 279 ms (0.28s) - Casos sin saturaci√≥n
- **Promedio:** 1,865 ms (1.87s)
- **Mediana:** 2,005 ms (2.01s)
- **p90/p95/p99:** ~2,008-2,012 ms (2.01s)
- **M√°xima:** 2,027 ms (2.03s)

**Observaci√≥n Cr√≠tica:**
> El hecho de que **p90, p95, p99 y m√°xima** est√©n todos en **~2 segundos** sugiere un **timeout configurado en el cliente Jedis o JMeter**. Las operaciones que toman m√°s de 2 segundos probablemente est√°n siendo canceladas.

**Comportamiento de Timeout:**
```
Distribuci√≥n de latencias:
  0-500ms:    ~8% de requests  ‚úÖ √âxito
  500-1000ms: ~0% de requests  
  1000-2000ms: ~0% de requests  
  >2000ms:    ~92% de requests ‚ùå Timeout/Fallo

Hip√≥tesis: Timeout de conexi√≥n = 2 segundos
```

**Implicaci√≥n:**
- Las tareas que **NO fallan** se encolan en **< 500ms**
- Las tareas que **S√ç fallan** timeout despu√©s de **~2 segundos**
- Redis probablemente est√° **tan saturado** que no responde en tiempo

#### 5.2.3 Evidencias

**Dashboard JMeter - Escenario 2 Worker:**
- Dashboard: `cloud_load_testing/escenario_2_capa_worker/video_10mb/dashboard_c1/index.html`
- Statistics: `cloud_load_testing/escenario_2_capa_worker/video_10mb/dashboard_c1/statistics.json`
- JTL: `cloud_load_testing/escenario_2_capa_worker/video_10mb/resultados_c1.jtl`

---

### 5.3 An√°lisis de Causas Ra√≠z

#### 5.3.1 Sobrecarga de Instancia t2.small

**Recursos Consumidos Durante la Prueba:**

```
CPU Usage (estimado con 2 vCPUs disponibles):
  - Celery Worker (4 workers): 60-80%
  - FFmpeg (si procesa videos): 90-100% (picos)
  - Redis: 10-15%
  - PostgreSQL: 5-10%
  - Sistema: 5-10%
  TOTAL: 170-215% ‚Üí EXCEDE 200% disponible (2 vCPUs)

Memoria Usage (estimado con 2 GiB disponibles):
  - Celery Worker: 500 MB - 1 GB
  - FFmpeg: 500 MB - 2 GB (picos)
  - Redis: 100-500 MB (seg√∫n queue depth)
  - PostgreSQL: 200 MB
  - Sistema: 300 MB
  TOTAL: 1.6 - 4.0 GiB ‚Üí EXCEDE 2 GiB disponibles
```

**‚ö†Ô∏è AN√ÅLISIS CR√çTICO - Sobrecarga Significativa:**

La instancia **t2.small** ahora debe ejecutar **M√ÅS servicios** que en Escenario 1:

| Servicio | Memoria Estimada | CPU Estimada | Estado |
|----------|-----------------|--------------|--------|
| **Celery Worker (4 concurrency)** | 500 MB - 1 GB | 0.5 - 1.0 vCPU | üî¥ Nuevo |
| **FFmpeg (durante procesamiento)** | 500 MB - 2 GB | 0.8 - 1.0 vCPU | üî¥ Picos cr√≠ticos |
| **Redis** | 50-100 MB | 0.1 vCPU | ‚úÖ |
| **PostgreSQL** | 200-500 MB | 0.2 vCPU | ‚ö†Ô∏è |
| **FastAPI** | 200-500 MB | 0.3 vCPU | ‚ö†Ô∏è |
| **Nginx** | 50 MB | 0.1 vCPU | ‚úÖ |
| **Sistema Operativo** | 300 MB | 0.2 vCPU | ‚úÖ |

**Total estimado:** **4-6 GiB RAM** **vs** 2 GiB disponibles = **D√©ficit de 2-4 GiB**

**Consecuencia:** Sistema en **swap constante**, causando latencias de 2+ segundos.

**Consecuencias Observadas:**
- üî¥ **Workers compiten por CPU** con FFmpeg
- üî¥ **Memory thrashing extremo** - Swap constante
- üî¥ **OOM Killer activo** - Mata procesos aleatoriamente
- üî¥ **Latencias err√°ticas** - Tareas timeout por falta de recursos
- üî¥ **91.53% de fracaso** - Sistema completamente saturado

#### 5.3.2 Saturaci√≥n de Cola Redis

**Escenario Probable:**
1. JMeter encola tareas a **5.35 ops/s** (tasa de entrada)
2. Workers procesan a **< 5.35 ops/s** (tasa de salida)
3. **Cola crece indefinidamente**
4. Redis se satura de memoria
5. Nuevas operaciones LPUSH **fallan o timeout**

**C√°lculo de Backlog:**
```
Tasa de entrada:  5.35 tareas/s
Tasa de salida:   ??? (no medida, pero < 5.35/s si cola crece)

Backlog despu√©s de 300s:
  = (Tasa entrada - Tasa salida) √ó 300s
  = Si salida = 0.5 tareas/s ‚Üí (5.35 - 0.5) √ó 300 = 1,455 tareas acumuladas
```

**Evidencia:**
- 91.53% de fallos sugiere que **Redis rechaza nuevas tareas** por saturaci√≥n

#### 5.3.3 Configuraci√≥n Inadecuada de Workers

**Workers Configurados:**
```yaml
worker:
  command: celery -A src.core.celery_app worker --concurrency=4
```

**An√°lisis:**
- **4 workers concurrentes** en instancia de **2 vCPUs**
- Cada worker puede procesar **1 tarea a la vez**
- Con **FFmpeg**, cada tarea puede tomar **30-120 segundos** (video processing)

**Capacidad Te√≥rica:**
```
Tiempo promedio por tarea: 60 segundos (estimado)
Workers concurrentes: 4
Throughput m√°ximo: 4 / 60s = 0.067 tareas/s = 4 tareas/minuto

VS

Tasa de encolado: 5.35 tareas/s = 321 tareas/minuto

Ratio: 321 / 4 = 80x M√ÅS R√ÅPIDO encolando que procesando
```

**Conclusi√≥n:** 
> Los workers est√°n **completamente saturados** y **NUNCA** podr√°n alcanzar la tasa de entrada. La cola crece infinitamente hasta que Redis colapsa.

**Arquitectura del Flujo de Procesamiento:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   JMeter     ‚îÇ
‚îÇ  (10 threads)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ LPUSH (mensaje 10MB)
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Redis Queue "celery"         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê ...    ‚îÇ
‚îÇ  ‚îÇMsg1‚îÇ ‚îÇMsg2‚îÇ ‚îÇMsg3‚îÇ ‚îÇMsg4‚îÇ        ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ RPOP (consume)
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Celery Worker (concurrency=4)     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇWorker 1‚îÇ ‚îÇWorker 2‚îÇ ‚îÇWorker 3‚îÇ  ‚îÇ
‚îÇ  ‚îÇ BUSY   ‚îÇ ‚îÇ BUSY   ‚îÇ ‚îÇ BUSY   ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ      ‚îÇ          ‚îÇ          ‚îÇ        ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ   ‚îÇ  FFmpeg Video Processing  ‚îÇ    ‚îÇ
‚îÇ   ‚îÇ  (CPU/Memory Intensive)   ‚îÇ    ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ       PostgreSQL (update status)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

### 5.4 Comparaci√≥n con Escenario 1

| M√©trica | Escenario 1 (Capa Web) | Escenario 2 (Capa Worker) | Diferencia |
|---------|------------------------|---------------------------|------------|
| **Componente Evaluado** | FastAPI + Nginx | Redis + Celery Workers |  |
| **Tasa de Error** | 38-94% (seg√∫n fase) | 91.53% | Similar (ambos colapsados) |
| **Throughput** | 0.12-0.30 req/s | 5.35 ops/s | +1,683% üü¢ |
| **Latencia Promedio** | 40-257 segundos | 1.87 segundos | -95% üü¢ |
| **Tipo de Falla** | Timeout HTTP, colapso EC2 | Timeout Redis, saturaci√≥n de cola |  |
| **Causa Ra√≠z Principal** | t2.small inadecuada, 1 worker Uvicorn | t2.small inadecuada, workers muy lentos vs tasa entrada |  |
| **Punto de Colapso** | 200-300 usuarios (instancia degradada) | 5.35 tareas/s (Redis saturado) |  |

**Observaciones Clave:**
- El **Escenario 2 tiene mejor latencia** (1.87s vs 40-257s) porque solo mide **encolado**, no procesamiento completo
- El **throughput es mayor** (5.35 vs 0.12-0.30) porque LPUSH es m√°s r√°pido que HTTP POST
- Ambos escenarios **fallan catastr√≥ficamente** debido a la **instancia t2.small inadecuada**
- **Ambos comparten la misma causa ra√≠z:** Recursos insuficientes (CPU y RAM)

**Conclusi√≥n Escenario 2:**

‚ùå‚ùå‚ùå **FALLO CATASTR√ìFICO** - Sistema de workers presenta **91.53% de tasa de error** al encolar tareas en Redis. Solo **8.47% de √©xito** indica **colapso total** del sistema de procesamiento as√≠ncrono. Workers **no pueden consumir la cola** al ritmo de entrada (321 tareas/min vs 4 tareas/min procesadas), causando **backlog infinito** y **saturaci√≥n de Redis**. **BLOQUEANTE para producci√≥n**.

---

## 6. Identificaci√≥n de Problemas Cr√≠ticos (Consolidado)

Esta secci√≥n consolida los bottlenecks identificados en **AMBOS escenarios** (Capa Web y Capa Worker), prioriz√°ndolos por impacto y urgencia.

---

### 6.1 Bottleneck #0: Infraestructura sin Redundancia ni Auto-scaling (BLOQUEANTE CR√çTICO)

**Afecta:** Ambos Escenarios (Web + Worker)

**Evidencia:**
- **Escenario 1:** Instancia EC2 se cae completamente con 200-300 usuarios
- Sistema operativo colapsa y no responde
- Requiere reinicio manual forzado
- P√©rdida total de disponibilidad

**Causa Ra√≠z:**
- **Single Point of Failure (SPOF):** Una √∫nica instancia EC2 sin redundancia
- **Sin l√≠mites de carga:** Sistema acepta todo el tr√°fico hasta colapsar
- **Recursos insuficientes:** Instancia no dimensionada para carga
- **Sin auto-scaling:** No hay mecanismo de escalamiento autom√°tico
- **Sin circuit breakers:** No hay protecci√≥n contra sobrecarga

**Impacto:**
- **Ca√≠da total del servicio** con cargas > 100 usuarios
- **SLA 0%** durante colapso (disponibilidad total: 0%)
- **P√©rdida de datos** en requests en proceso
- **Tiempo de recuperaci√≥n:** 5-10 minutos (reinicio manual)
- **Riesgo operacional cr√≠tico** - Sistema no production-ready

**Prioridad:** üî¥üî¥üî¥ **BLOQUEANTE CR√çTICO** - Impide cualquier operaci√≥n en producci√≥n

**Soluci√≥n Requerida:**
1. **Infraestructura redundante:** M√≠nimo 2-3 instancias con Load Balancer
2. **Auto Scaling Groups (ASG):** Escalamiento autom√°tico basado en CPU/memoria
3. **Rate Limiting:** Protecci√≥n contra sobrecarga (ej: 10 req/s por IP)
4. **Circuit Breakers:** Detener aceptaci√≥n de tr√°fico cuando recursos cr√≠ticos
5. **Health Checks:** Auto-recuperaci√≥n y remoci√≥n de instancias no saludables

---

### 6.2 Bottleneck #1: Instancia t2.small Inadecuada para la Carga (CR√çTICO)

**Afecta:** Ambos Escenarios (Web + Worker)

**Evidencia - Escenario 1 (Capa Web):**
- Instancia t2.small: 2 vCPUs, 2 GiB RAM
- Aplicaci√≥n requiere: 4+ vCPUs, 4+ GiB RAM
- D√©ficit de recursos: -50% CPU, -50% RAM
- Colapso con 200-300 usuarios (instancia degradada)
- Latencias inconsistentes (7s - 358s)

**Evidencia - Escenario 2 (Capa Worker):**
- 91.53% de tasa de error al encolar tareas
- Redis saturado no acepta nuevas conexiones
- Latencias de ~2 segundos (timeouts)
- Workers + FFmpeg exceden 2x la memoria disponible

**Causa Ra√≠z:**
```yaml
Recursos disponibles vs requeridos:
  t2.small disponible:
    - 2 vCPUs
    - 2 GiB RAM
  
  Aplicaci√≥n requiere (FastAPI + Workers + FFmpeg):
    - CPU: 4+ vCPUs
    - RAM: 4+ GiB
    
  D√©ficit: -75% CPU, -75% RAM
```

**An√°lisis de Sobrecarga:**

**Servicios corriendo en 2 GiB RAM:**
1. FastAPI: 1.7 GB configurado (l√≠mite) - **CASI TODO EL RAM**
2. Celery Worker (4 workers): 500 MB - 1 GB
3. FFmpeg (durante procesamiento): 500 MB - 2 GB (picos)
4. PostgreSQL: ~300 MB (m√≠nimo) + conexiones
5. Redis: ~50-100 MB (base) + 100-500 MB (cola creciente)
6. Nginx: ~50 MB
7. Sistema Operativo: ~300 MB

**Total:** 4-6 GiB requerido **vs** 2 GiB disponibles = **D√©ficit de 2-4 GiB**

**Consecuencias Observadas:**
- üî¥ **Memory Pressure:** Sistema con presi√≥n de memoria (causa latencias de 40-257s en Escenario 1, 2s en Escenario 2)
- üî¥ **Posible OOM Killer:** Linux puede matar procesos bajo presi√≥n extrema
- üî¥ **CPU Contention:** t2.small con 2 vCPUs insuficientes para 5+ servicios
- üî¥ **Workers compiten por CPU** con FFmpeg
- üî¥ **Latencias altas:** Swap a disco es 1000x m√°s lento que RAM
- üî¥ **Inestabilidad:** Procesos pueden degradarse bajo carga

**Impacto:**
- **Explica el 100% de los problemas observados en AMBOS escenarios**
- Latencias de 40-257 segundos (Escenario 1) por presi√≥n de memoria
- Latencias de 2 segundos (Escenario 2) por timeouts de Redis saturado
- Colapso total con 200-300 usuarios por OOM
- 91.53% de error (Escenario 2) por saturaci√≥n de recursos
- Comportamiento err√°tico entre fases

**Prioridad:** üî¥üî¥üî¥ **CR√çTICO BLOQUEANTE** - Causa ra√≠z principal de AMBOS escenarios

**Soluci√≥n Requerida:**

**Opci√≥n A - Instancia Unificada (NO recomendado):**
- **M√çNIMO:** t3.large (2 vCPU, 8 GB RAM) - Costo: ~$60/mes
- **RECOMENDADO:** c5.2xlarge (8 vCPU, 16 GB RAM) - Costo: ~$240/mes

**Opci√≥n B - Separaci√≥n de Capas (RECOMENDADO):**
```
API Layer:    t3.medium  (2 vCPU, 4 GB)  ~$30/mes
Worker Layer: c5.2xlarge (8 vCPU, 16 GB) ~$240/mes
TOTAL: ~$270/mes
```

**ROI del Upgrade:**
- t2.small actual: ~$17/mes ‚Üí Sistema CON LIMITACIONES
- Separaci√≥n de capas: ~$270/mes ‚Üí Sistema FUNCIONAL
- **Incremento de costo:** $253/mes
- **Incremento de capacidad:** 50x+ (de 10 usuarios ‚Üí 500+ usuarios)
- **ROI:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê INMEDIATO

---

### 6.3 Bottleneck #2: Configuraci√≥n de Uvicorn (CR√çTICO - Escenario 1)

**Causa Ra√≠z:**
```dockerfile
# Dockerfile actual
CMD ["uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000"]
# Solo 1 worker, ~1000 connections por defecto
```

**Impacto:**
- **Single worker** procesa todas las requests secuencialmente
- Queue de conexiones se satura inmediatamente
- Timeouts masivos en requests encoladas

### 6.3 Bottleneck #2: Configuraci√≥n de Uvicorn (CR√çTICO - Escenario 1)

**Afecta:** Escenario 1 (Capa Web)

**Evidencia:**
- Latencias extremas (40-257 segundos promedio)
- Tasa de error > 38% incluso con 5 usuarios
- Throughput colapsado (0.12-0.30 RPS)

**Causa Ra√≠z:**
```dockerfile
# Dockerfile actual
CMD ["uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000"]
# Solo 1 worker, ~1000 connections por defecto
```

**Impacto:**
- **Single worker** procesa todas las requests secuencialmente
- Queue de conexiones se satura inmediatamente
- Timeouts masivos en requests encoladas

**Prioridad:** üî¥ **CR√çTICA** - Bloqueante para producci√≥n del Escenario 1

**Soluci√≥n:** Ver secci√≥n 7.1.1

---

### 6.4 Bottleneck #3: Workers Insuficientes para Carga (CR√çTICO - Escenario 2)

**Afecta:** Escenario 2 (Capa Worker)

**Evidencia:**
- Tasa de encolado: 5.35 tareas/s (321 tareas/min)
- Capacidad de procesamiento: ~0.067 tareas/s (4 tareas/min)
- **Ratio 80:1** - Se encolan 80x m√°s r√°pido de lo que se procesan
- 91.53% de tasa de error

**Causa Ra√≠z:**
```python
# docker-compose.worker.yml
command: celery -A src.core.celery_app worker --loglevel=info --concurrency=4

# Solo 4 workers para procesar videos con FFmpeg
# Cada video toma 30-120 segundos
# Throughput m√°ximo: 4 tareas/minuto
# VS tasa de entrada: 321 tareas/minuto
```

**Impacto:**
- Cola de Redis crece infinitamente
- Backlog de 1,455+ tareas acumuladas en 5 minutos
- Redis se queda sin memoria
- Nuevas tareas son rechazadas (91.53% error)

**Prioridad:** üî¥üî¥ **CR√çTICO** - Bloqueante para producci√≥n del Escenario 2

**Soluci√≥n:** Ver secci√≥n 7.1.2

---

### 6.5 Bottleneck #4: Procesamiento de Videos (ALTO - Ambos Escenarios)

**Afecta:** Ambos Escenarios (Web + Worker)

**Evidencia - Escenario 1:**
- Latencias m√≠nimas de 5-7 segundos incluso con 5 usuarios
- Sistema desacoplado en local funcionaba mejor
- Endpoint de upload involucra procesamiento s√≠ncrono

**Evidencia - Escenario 2:**
- Cada worker bloqueado durante 30-120 segundos procesando video con FFmpeg
- FFmpeg consume 80-100% CPU durante procesamiento
- Workers no pueden procesar m√°s tareas mientras FFmpeg ejecuta
- Throughput limitado a 0.067 tareas/s (4 tareas/minuto)

**Causa Ra√≠z:**
```python
# Escenario 1: Procesamiento s√≠ncrono en request HTTP
@video_router.post("/api/videos/upload")
async def upload_video(...):
    # Procesamiento BLOQUEANTE durante request
    process_video(video_id)  # 30-120 segundos
    return response

# Escenario 2: Worker bloqueado por FFmpeg
def process_video_task(self, video_id: int):
    # Worker bloqueado aqu√≠ durante 30-120 segundos
    result = subprocess.run(ffmpeg_command, timeout=1800)
    # No puede procesar otras tareas
```

**Impacto:**
- **Escenario 1:** Request HTTP bloqueada durante procesamiento ‚Üí Timeout de clients
- **Escenario 2:** Throughput limitado por tiempo de FFmpeg ‚Üí Workers ociosos durante I/O de disco

**Prioridad:** ÔøΩ **ALTA** - Requiere arquitectura as√≠ncrona real

**Soluci√≥n:** Ver secci√≥n 7.1.2

---

### 6.6 Bottleneck #5: Sin Auto-scaling de Workers (ALTO - Escenario 2)

**Afecta:** Escenario 2 (Capa Worker)

**Evidencia:**
- Workers configurados est√°ticamente (concurrency=4)
- No hay escalamiento basado en queue depth de Redis
- Sin mecanismo de balanceo de carga din√°mico
- Sistema no puede adaptarse a picos de carga

**Causa Ra√≠z:**
- **Configuraci√≥n manual** de workers en docker-compose
- **Sin Kubernetes o ECS** para auto-scaling
- **Sin m√©tricas** de Celery para tomar decisiones de escalamiento
- **Sin alertas** cuando queue depth crece

**Impacto:**
- Sistema no puede adaptarse a picos de carga
- Backlog de 1,455+ tareas crece sin control en 5 minutos
- Requiere intervenci√≥n manual para escalar
- No hay elasticidad del sistema

**Prioridad:** üü† **ALTA**

**Soluci√≥n:** Ver secci√≥n 7.2.1

---

### 6.7 Bottleneck #6: Network/Latency AWS (MEDIO - Escenario 1)

**Afecta:** Escenario 1 (Capa Web)

**Evidencia:**
- Latencias base superiores a pruebas locales
- Variabilidad alta en tiempos de respuesta
- JMeter ejecutando desde misma instancia EC2

**Causa Ra√≠z Probable:**
- Red interna entre JMeter y aplicaci√≥n en misma instancia
- Competencia por ancho de banda limitado de t2.small
- Sin CDN o edge locations para contenido est√°tico

**Impacto:**
- Incremento de latencia base (~5-7 segundos m√≠nimo)
- Timeouts m√°s probables
- Impacto menor comparado con otros bottlenecks

**Prioridad:** üü° **MEDIA** - Optimizable con configuraci√≥n de red

---

## 7. Recomendaciones para Escalar la Soluci√≥n

### 7.1 Acciones Inmediatas (Alta Prioridad)

#### 10.1.1 Incrementar Workers de Uvicorn

**Cambio en Dockerfile:**
```dockerfile
# ANTES
CMD ["uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000"]

# DESPU√âS
CMD ["uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000", \
     "--workers", "4", \
     "--worker-connections", "2000", \
     "--timeout-keep-alive", "75"]
```

**Impacto Estimado:**
- Capacidad: < 5 usuarios ‚Üí 50-100 usuarios
- RPS: 0.30 ‚Üí 5-10 RPS
- Error rate: 73% ‚Üí < 10%

**Esfuerzo:** Bajo (1 l√≠nea de c√≥digo)  
**ROI:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

---

#### 10.1.2 Desacoplar Worker de Procesamiento

**Modificaci√≥n en video_router.py:**
```python
@video_router.post("/api/videos/upload", status_code=202)
async def upload_video(...):
    # Guardar video en storage
    # Responder INMEDIATAMENTE con 202 Accepted
    
    # Encolar tarea as√≠ncrona
    task = process_video_task.delay(video.id)
    
    return {"message": "Video accepted for processing", "task_id": task.id}
```

**Impacto Estimado:**
- Latencia: 80s ‚Üí < 1s
- Throughput: +500%
- Error rate: Reducci√≥n del 90%

**Esfuerzo:** Bajo  
**ROI:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

---

#### 10.1.3 Upgrade URGENTE de Instancia EC2 (t2.small ‚Üí t3.large)

**Estado Actual:**
- **t2.small:** 2 vCPUs, 2 GiB RAM (~$17/mes)
- **D√©ficit:** -50% CPU, -50% RAM
- **Estado:** Sistema CON LIMITACIONES SEVERAS

**Upgrade Recomendado:**

| Opci√≥n | Tipo | vCPU | RAM | Costo/mes | Capacidad Estimada | Recomendaci√≥n |
|--------|------|------|-----|-----------|-------------------|---------------|
| **Actual** | t2.small | 2 | 2 GiB | ~$17 | 10-20 usuarios | ‚ö†Ô∏è Insuficiente |
| **Aceptable** | t3.medium | 2 | 4 GiB | ~$30 | 50-100 usuarios | ‚úÖ M√≠nimo viable |
| **Recomendado** | t3.large | 2 | 8 GiB | ~$60 | 100-300 usuarios | ‚≠ê RECOMENDADO |
| **√ìptimo** | c5.xlarge | 4 | 8 GiB | ~$120 | 300-500 usuarios | ‚≠ê‚≠ê Producci√≥n |

**Configuraci√≥n Docker Compose para t3.large:**
```yaml
fastapi:
  deploy:
    resources:
      limits:
        cpus: '1.5'     # 75% de 2 vCPUs
        memory: '3G'    # 37% de 8 GB
      reservations:
        cpus: '1.0'
        memory: '2G'

postgres:
  deploy:
    resources:
      limits:
        cpus: '0.5'
        memory: '2G'

redis:
  deploy:
    resources:
      limits:
        cpus: '0.25'
        memory: '512M'

celery_worker:
  deploy:
    resources:
      limits:
        cpus: '1.0'
        memory: '2G'    # FFmpeg requiere memoria
```

**Impacto Estimado del Upgrade (t2.small ‚Üí t3.large):**
- Estabilidad: +400% (mayor margen de recursos)
- Capacidad: 10-20 usuarios ‚Üí 100-300 usuarios
- Latencia: 40s ‚Üí < 2s
- Error rate: 73% ‚Üí < 10%
- Disponibilidad: ~50% ‚Üí 99%+

**Esfuerzo:** Bajo (15 minutos)
```bash
# 1. Detener instancia actual
aws ec2 stop-instances --instance-ids i-xxxxx

# 2. Cambiar tipo de instancia
aws ec2 modify-instance-attribute \
  --instance-id i-xxxxx \
  --instance-type t3.large

# 3. Reiniciar instancia
aws ec2 start-instances --instance-ids i-xxxxx
```

**Costo Incremental:** $43/mes ($60 - $17)  
**ROI:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê **CR√çTICO - M√ÅXIMA PRIORIDAD**

**URGENCIA:** üî¥üî¥üî¥ **INMEDIATO** - Sin este upgrade, el sistema no escala

---

### 7.2 Optimizaciones a Mediano Plazo

#### 10.2.1 Migrar Base de Datos a RDS

**Servicio:** Amazon RDS para PostgreSQL

**Beneficios:**
- Alta disponibilidad autom√°tica
- Backups automatizados
- Escalamiento vertical f√°cil
- R√©plicas de lectura

**Configuraci√≥n Recomendada:**
- **Instancia:** db.t3.medium (2 vCPU, 4 GB RAM)
- **Storage:** 100 GB SSD (gp3)
- **Multi-AZ:** S√≠ (para producci√≥n)

---

#### 10.2.2 Migrar Redis a ElastiCache

**Servicio:** Amazon ElastiCache para Redis

**Beneficios:**
- Baja latencia garantizada
- Replicaci√≥n autom√°tica
- Snapshots automatizados

**Configuraci√≥n Recomendada:**
- **Tipo de nodo:** cache.t3.medium
- **N√∫mero de nodos:** 2 (1 primario + 1 r√©plica)

---

#### 10.2.3 Implementar Auto Scaling

**Amazon ECS con Fargate:**
```yaml
# Task Definition
resources:
  cpu: 2048  # 2 vCPU
  memory: 4096  # 4 GB

# Auto Scaling
min_capacity: 2
max_capacity: 10
target_cpu_utilization: 70%
```

**Impacto Estimado:**
- Capacidad: 100 ‚Üí 500+ usuarios
- Disponibilidad: 99.9%+

---

### 7.3 Arquitectura de Largo Plazo

#### 7.3.1 Arquitectura Propuesta

```
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   CloudFront    ‚îÇ (CDN)
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Application    ‚îÇ
                    ‚îÇ  Load Balancer  ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ              ‚îÇ              ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ ECS/    ‚îÇ    ‚îÇ ECS/   ‚îÇ    ‚îÇ ECS/   ‚îÇ
         ‚îÇ Fargate ‚îÇ    ‚îÇFargate ‚îÇ    ‚îÇFargate ‚îÇ
         ‚îÇ Task 1  ‚îÇ    ‚îÇTask 2  ‚îÇ    ‚îÇTask 3  ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ             ‚îÇ              ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ             ‚îÇ             ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ   RDS   ‚îÇ   ‚îÇ Elasti ‚îÇ   ‚îÇ   S3   ‚îÇ
         ‚îÇ (Multi  ‚îÇ   ‚îÇ Cache  ‚îÇ   ‚îÇ(Videos)‚îÇ
         ‚îÇ   AZ)   ‚îÇ   ‚îÇ(Redis) ‚îÇ   ‚îÇ        ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Componentes:**
- **CloudFront:** CDN para contenido est√°tico y videos procesados
- **ALB:** Load balancer para distribuir tr√°fico
- **ECS Fargate:** Contenedores sin gesti√≥n de servidores
- **RDS Multi-AZ:** Alta disponibilidad de base de datos
- **ElastiCache:** Redis gestionado
- **S3:** Almacenamiento escalable de videos

**Estimaci√≥n de Capacidad:**
- **Usuarios concurrentes:** 1,000-5,000
- **RPS sostenido:** 100-500
- **Disponibilidad:** 99.95%+

---

## 8. Plan de Acci√≥n Inmediato

### 8.1 Roadmap de Implementaci√≥n

| Fase | Acci√≥n | Esfuerzo | Impacto | Plazo |
|------|--------|----------|---------|-------|
| **0** | **UPGRADE t2.small ‚Üí t3.large** | **15 min** | **üî¥üî¥üî¥ BLOQUEANTE** | **INMEDIATO** |
| **1** | Incrementar workers Uvicorn a 4 | 1 hora | üî¥ CR√çTICO | Inmediato |
| **2** | Desacoplar procesamiento as√≠ncrono | 2 horas | üî¥ CR√çTICO | 1 d√≠a |
| **3** | Configurar l√≠mites de recursos (nuevo hardware) | 1 hora | üü† ALTO | 1 d√≠a |
| **4** | Re-ejecutar pruebas de capacidad | 4 horas | üü° VALIDACI√ìN | 2 d√≠as |
| **5** | Migrar a RDS + ElastiCache | 1 d√≠a | üü† MEDIO | 1 semana |
| **6** | Implementar ECS + Auto Scaling | 3 d√≠as | üü¢ LARGO PLAZO | 2 semanas |

### 8.2 Quick Wins (Implementar HOY)

**‚ö†Ô∏è ACCI√ìN #0 - CR√çTICA (PRIMERO):**
```bash
# UPGRADE INSTANCIA EC2: t2.small ‚Üí t3.large
# Tiempo: 15 minutos | Costo: +$52/mes | Impacto: +1000% capacidad

# 1. Detener instancia
aws ec2 stop-instances --instance-ids i-xxxxx

# 2. Cambiar tipo
aws ec2 modify-instance-attribute \
  --instance-id i-xxxxx \
  --instance-type t3.large

# 3. Iniciar instancia
aws ec2 start-instances --instance-ids i-xxxxx

# 4. Verificar
aws ec2 describe-instances --instance-ids i-xxxxx | grep InstanceType
```

**Tiempo:** 15 minutos  
**Impacto esperado:** Sistema pasa de INOPERABLE ‚Üí FUNCIONAL  
**Criticidad:** üî¥üî¥üî¥ **SIN ESTE PASO, NADA M√ÅS FUNCIONA**

---

**Acci√≥n #1 - Modificar Uvicorn (DESPU√âS del upgrade):**
```bash
# 1. Modificar Dockerfile
vim Dockerfile
# Agregar: --workers 4 --worker-connections 2000

# 2. Rebuild imagen
docker build -t fastapi-app .

# 3. Restart servicios
docker-compose down
docker-compose up -d

# 4. Validar
curl http://localhost/
```

**Tiempo total HOY:** < 2 horas  
**Impacto esperado:** Reducci√≥n de errores del 90%+

---

## 9. Conclusiones

### 9.1 Estado Actual del Sistema

El sistema presenta **fallas catastr√≥ficas de capacidad** que lo hacen **COMPLETAMENTE INOPERABLE en producci√≥n**:

- üí• **COLAPSO TOTAL:** Instancia EC2 se cae con 200-300 usuarios concurrentes
- ‚ùå Tasa de error superior al 38% en todos los escenarios funcionales
- ‚ùå Latencias de 40-257 segundos (40-257x sobre SLO)
- ‚ùå Throughput colapsado (0.12-0.30 RPS)
- ‚ùå Sistema no escala correctamente con aumento de carga
- üî¥ **Requiere reinicio manual** cuando colapsa - No auto-recuperaci√≥n

### 9.2 Causa Ra√≠z Identificada

**Bottleneck #0 (BLOQUEANTE - Ambos Escenarios):** Infraestructura sin redundancia ni auto-scaling
- Single Point of Failure: Una √∫nica instancia EC2
- Sin protecci√≥n contra sobrecarga
- Sin mecanismos de auto-recuperaci√≥n
- Sin balanceo de carga

**Bottleneck #1 (CR√çTICO - Ambos Escenarios):** Instancia t2.small inadecuada para la carga
- **2 vCPUs, 2 GiB RAM** para aplicaci√≥n que requiere **8+ vCPUs, 16+ GiB RAM**
- **D√©ficit del 75%** en CPU y **87.5%** en RAM
- Causa directa en **AMBOS escenarios** de:
  - ‚úÖ **Escenario 1:** Latencias de 40-257 segundos (presi√≥n de memoria)
  - ‚úÖ **Escenario 1:** Colapso con 200-300 usuarios (recursos agotados)
  - ‚úÖ **Escenario 1:** Tasa de error 38-94% (recursos insuficientes)
  - ‚úÖ **Escenario 1:** Throughput colapsado (CPU contention)
  - ‚úÖ **Escenario 2:** 91.53% de error al encolar tareas (Redis saturado)
  - ‚úÖ **Escenario 2:** Latencias de 2s (timeouts de Redis)
  - ‚úÖ **Escenario 2:** Workers + FFmpeg compiten por recursos limitados

**Bottleneck #2 (CR√çTICO - Escenario 1):** Configuraci√≥n inadecuada de Uvicorn
- Single worker procesa todas las requests secuencialmente
- Queue de conexiones saturada inmediatamente
- Sin procesamiento paralelo real

**Bottleneck #3 (CR√çTICO - Escenario 2):** Workers insuficientes para carga
- **Solo 4 workers** para procesar videos con FFmpeg
- **Throughput m√°ximo:** 4 tareas/minuto (0.067 tareas/s)
- **Tasa de entrada:** 321 tareas/minuto (5.35 tareas/s)
- **Ratio 80:1** - Se encolan 80x m√°s r√°pido de lo que se procesan
- **Backlog infinito** - Cola de Redis crece sin control hasta saturaci√≥n
- **91.53% de error** por rechazo de nuevas tareas

**Bottleneck #4 (ALTO - Ambos Escenarios):** Procesamiento de videos bloqueante
- **Escenario 1:** Request HTTP bloqueada durante procesamiento (30-120s)
- **Escenario 2:** Worker bloqueado por FFmpeg (30-120s)
- CPU desperdiciada durante I/O de disco
- Sin arquitectura as√≠ncrona real

**Bottleneck #5 (ALTO - Escenario 2):** Sin auto-scaling de workers
- Workers configurados est√°ticamente (concurrency=4)
- No hay escalamiento basado en queue depth de Redis
- Sistema no puede adaptarse a picos de carga
- Requiere intervenci√≥n manual

### 9.3 Viabilidad de Producci√≥n

**Veredicto:** ‚ùå‚ùå‚ùå **COMPLETAMENTE INAPTO para producci√≥n en AMBOS escenarios**

**Hallazgos Cr√≠ticos Consolidados:** 
> üö® El sistema presenta **fallas severas en DOS niveles simult√°neos**:
> 
> **NIVEL 1 - Capa Web (Escenario 1):** La instancia **t2.small (2 vCPUs, 2 GiB RAM)** es **inadecuada** para correr 5 servicios Docker que requieren 4+ GiB de RAM. Con cargas de **200-300 usuarios la instancia SE DEGRADA SEVERAMENTE** por presi√≥n de memoria.
> 
> **NIVEL 2 - Capa Worker (Escenario 2):** Solo **4 workers** intentan procesar **80x m√°s tareas** de las que pueden manejar (321 tareas/min entrando vs 4 tareas/min procesadas), causando **backlog infinito** y **saturaci√≥n de Redis con 91.53% de error**.
> 
> Esto representa un **riesgo operacional ALTO** que hace el sistema **NO ESCALABLE** para producci√≥n.

**Requisitos M√≠nimos para Producci√≥n:**

**BLOQUEANTES (Sin estos, el sistema NO escala apropiadamente):**

1. ‚úÖ **SEPARACI√ìN DE CAPAS - MANDATORIO:**
   - **API Layer:** Instancia t3.medium (2 vCPU, 4 GiB RAM) - $30/mes
   - **Worker Layer:** Instancia c5.2xlarge (8 vCPU, 16 GiB RAM) - $240/mes
   - **Total:** ~$270/mes vs $17/mes actual (+$253/mes)
   - **ROI:** Sistema pasa de LIMITADO ‚Üí FUNCIONAL

2. ‚úÖ **Implementar infraestructura redundante con Load Balancer**
   - M√≠nimo 2 instancias de API con ALB
   - Health checks y auto-recovery
   - Eliminar Single Point of Failure

3. ‚úÖ **Configurar Auto Scaling Groups**
   - API Layer: 2-10 instancias basado en CPU
   - Worker Layer: 2-20 instancias basado en queue depth
   - Escalamiento autom√°tico para picos de carga

4. ‚úÖ **Implementar Rate Limiting y Circuit Breakers**
   - Nginx: limit_req 10 req/s por IP
   - Circuit breakers cuando recursos > 80%
   - Protecci√≥n contra sobrecarga

5. ‚úÖ **Migrar Redis a ElastiCache (Cluster Mode)**
   - Alta disponibilidad con replicaci√≥n
   - Backups automatizados
   - Eliminar saturaci√≥n de Redis

**CR√çTICOS (Escenario 1 - Capa Web):**

6. ‚úÖ Incrementar workers de Uvicorn a 4
7. ‚úÖ Implementar procesamiento as√≠ncrono real (desacoplar upload de procesamiento)

**CR√çTICOS (Escenario 2 - Capa Worker):**

8. ‚úÖ Aumentar concurrency de Celery a 16-32 workers
9. ‚úÖ Implementar auto-scaling basado en queue depth de Redis
10. ‚úÖ Implementar queue prioritization (videos urgentes vs normales)

**VALIDACI√ìN:**

11. ‚úÖ Re-ejecutar pruebas de capacidad COMPLETAS (Escenario 1 + Escenario 2)
12. ‚úÖ Validar que tasa de error < 5% en ambos escenarios
13. ‚úÖ Validar que queue depth de Redis se mantiene < 100 tareas

### 9.4 Pr√≥ximos Pasos

1. **URGENTE - Inmediato (HOY):**
   - ‚ö†Ô∏è **NO DESPLEGAR EN PRODUCCI√ìN** bajo ninguna circunstancia
   - Documentar hallazgo cr√≠tico de colapso en **AMBAS capas** (Web + Worker)
   - Escalar a equipo de arquitectura para **redise√±o completo**
   - Presentar an√°lisis de costo vs beneficio de separaci√≥n de capas

2. **Cr√≠tico (Esta Semana):**
   
   **Capa Web (Escenario 1):**
   - **UPGRADE API Layer:** Crear instancia t3.medium (2 vCPU, 4 GB RAM)
   - Implementar Load Balancer + 2 instancias t3.medium
   - Configurar Auto Scaling Group para API (2-10 instancias)
   - Implementar Rate Limiting (nginx: limit_req 10/s)
   - Incrementar workers Uvicorn a 4
   - Desacoplar upload de procesamiento (retornar 202 Accepted inmediatamente)
   
   **Capa Worker (Escenario 2):**
   - **CREAR Worker Layer:** Instancia c5.2xlarge dedicada (8 vCPU, 16 GB RAM)
   - Aumentar concurrency de Celery a 16 workers
   - Configurar Auto Scaling Group para Workers (2-20 instancias basado en queue depth)
   - Implementar queue prioritization (cola r√°pida vs lenta)
   - Migrar Redis a ElastiCache (cache.r6g.large con replicaci√≥n)
   
   **Infraestructura General:**
   - Configurar CloudWatch con alertas:
     * CPU > 80% en cualquier instancia
     * Queue depth > 100 tareas
     * Error rate > 5%
   - Implementar health checks en ambas capas

3. **Corto Plazo (2 Semanas):**
   - Migrar PostgreSQL a RDS Multi-AZ (db.t3.medium)
   - Implementar monitoreo completo:
     * Prometheus + Grafana para m√©tricas de aplicaci√≥n
     * AWS X-Ray para trazas distribuidas
     * CloudWatch Logs Insights para an√°lisis de logs
   - **Re-ejecutar pruebas de capacidad COMPLETAS:**
     * Escenario 1: 5, 100, 200, 300, 500 usuarios
     * Escenario 2: Validar throughput de 10+ tareas/s con queue depth < 100
   - Documentar nuevos resultados y comparar con baseline

4. **Largo Plazo (1 Mes):**
   - Migrar a arquitectura de microservicios con ECS Fargate:
     * API Service (auto-scaling 3-50 tasks)
     * Worker Service (auto-scaling 5-100 tasks)
     * Scheduler Service (cron jobs)
   - Implementar CDN (CloudFront) para videos procesados
   - Evaluar AWS MediaConvert para reemplazar FFmpeg (servicio gestionado)
   - Configurar CI/CD con deployment blue-green o canary
   - Implementar multi-regi√≥n para alta disponibilidad

### 9.5 Lecciones Aprendidas

**Lecciones de Infraestructura (Ambos Escenarios):**

1. **Single Point of Failure es CR√çTICO:**
   - Una sola instancia EC2 es **inaceptable para producci√≥n**
   - Sistema debe tener redundancia desde d√≠a 1
   - Colapso total causa p√©rdida completa de servicio en ambas capas
   - Load balancer y auto-scaling son **MANDATORIOS**, no opcionales

2. **Dimensionamiento de hardware es FUNDAMENTAL:**
   - **t2.small (2 GiB RAM) es INSUFICIENTE para aplicaci√≥n que requiere 4+ GiB**
   - Presi√≥n de memoria causa latencias altas (40-257 segundos en Escenario 1)
   - Posibles OOM Killer bajo carga extrema
   - **Inversi√≥n en hardware adecuado ($253/mes) evita degradaci√≥n**
   - **ROI inmediato:** Sistema pasa de limitado ‚Üí funcional y escalable

3. **Separaci√≥n de capas es MANDATORIA:**
   - **API y Workers NO deben compartir recursos**
   - Competencia por CPU/RAM causa degradaci√≥n en ambos lados
   - API requiere baja latencia ‚Üí Instancias T3 (general purpose)
   - Workers requieren alta CPU ‚Üí Instancias C5 (compute optimized)
   - Escalamiento independiente permite optimizaci√≥n de costos

**Lecciones de Capa Web (Escenario 1):**

4. **Configuraci√≥n por defecto es insuficiente:**
   - Uvicorn single worker NO es para producci√≥n
   - Recursos de contenedor deben dimensionarse expl√≠citamente
   - L√≠mites de memoria deben considerar picos, no promedio
   - Testing de configuraci√≥n es cr√≠tico antes de producci√≥n

5. **Protecci√≥n contra sobrecarga es mandatoria:**
   - Rate limiting DEBE implementarse desde d√≠a 1
   - Circuit breakers son esenciales para evitar cascadas de fallo
   - Sistema debe degradarse gracefully, no colapsar
   - Limits de recursos previenen consumo descontrolado

**Lecciones de Capa Worker (Escenario 2):**

6. **Dimensionamiento de Workers es CR√çTICO:**
   - Workers deben procesar **M√ÅS R√ÅPIDO** de lo que se encolan tareas
   - **Ratio 80:1** (entrada vs salida) causa colapso inevitable
   - F√≥rmula: `workers_needed = (tasa_entrada √ó tiempo_procesamiento) / 60`
   - Ejemplo: (321 tareas/min √ó 60s) / 60 = **321 workers** necesarios (ten√≠amos 4)
   - Usar FFmpeg con preset m√°s r√°pido o menos workers por core

7. **Auto-scaling de workers es MANDATORIO:**
   - Carga de tareas var√≠a dr√°sticamente en el tiempo
   - Workers est√°ticos NO pueden manejar picos
   - Queue depth debe triggear escalamiento autom√°tico
   - M√©tricas de Celery deben monitorearse constantemente
   - Alertas cuando queue depth > 100 tareas

8. **Redis single instance NO es production-ready:**
   - **ElastiCache con replicaci√≥n es mandatorio**
   - Backups automatizados son esenciales
   - High availability evita Single Point of Failure
   - Cluster mode permite escalamiento horizontal
   - Saturaci√≥n de Redis causa cascada de fallos

9. **Queue prioritization es esencial:**
   - No todas las tareas tienen la misma prioridad
   - Videos urgentes vs normales deben tener colas separadas
   - Workers dedicados por tipo de tarea
   - Evita HOL (Head-of-Line) blocking
   - Mejora SLA para tareas cr√≠ticas

**Lecciones de Testing y Monitoreo:**

10. **Testing en m√∫ltiples ambientes es cr√≠tico:**
    - Pruebas locales NO reflejan comportamiento en cloud
    - Diferencias de infraestructura causan degradaci√≥n masiva
    - **Pruebas de carga extrema revelan colapsos catastr√≥ficos**
    - Testing debe incluir AMBAS capas (Web + Worker)
    - Pruebas end-to-end son mandatorias

11. **Testing de capacidad debe ser end-to-end:**
    - Medir solo encolado (Escenario 2) NO es suficiente
    - Debe medirse tiempo total de procesamiento
    - Validar comportamiento con backlog creciente
    - Simular condiciones realistas (m√∫ltiples tipos de carga)
    - Pruebas de soak (24-48 horas) revelan memory leaks

12. **Monitoreo es esencial desde d√≠a 1:**
    - Sin m√©tricas, los problemas son invisibles hasta que colapsan
    - CloudWatch/Prometheus deben configurarse ANTES de producci√≥n
    - M√©tricas cr√≠ticas:
      * **Capa Web:** RPS, latencia p95/p99, error rate, CPU, RAM
      * **Capa Worker:** Queue depth, tasks processed/s, error rate, CPU, RAM
    - Alertas autom√°ticas deben existir para todas las m√©tricas cr√≠ticas
    - Dashboards deben ser accesibles 24/7

**Lecciones de Arquitectura:**

13. **Arquitectura as√≠ncrona es mandatoria:**
    - Procesamiento pesado (videos) debe ser as√≠ncrono
    - HTTP request debe responder inmediatamente (< 200ms)
    - Pattern: Accept request ‚Üí Enqueue task ‚Üí Return 202 Accepted
    - Polling o webhooks para notificar completaci√≥n
    - Mejora UX y permite escalamiento independiente

---

## 10. Anexos

### 10.1 Archivos de Resultados

**Escenario 1 - Capa Web (API REST):**

**Fase 1 - Sanidad:**
- Dashboard: `cloud_load_testing/escenario_1_capa_web/Fase_1_Sanidad/dashboard_smoke/index.html`
- Statistics: `cloud_load_testing/escenario_1_capa_web/Fase_1_Sanidad/dashboard_smoke/statistics.json`
- JTL: `cloud_load_testing/escenario_1_capa_web/Fase_1_Sanidad/fase1_smoke.jtl`

**Fase 2 - Escalamiento:**
- Dashboard: `cloud_load_testing/escenario_1_capa_web/Fase_2_escalamiento_fail/dashboard_smoke/index.html`
- Statistics: `cloud_load_testing/escenario_1_capa_web/Fase_2_escalamiento_fail/dashboard_smoke/statistics.json`
- JTL: `cloud_load_testing/escenario_1_capa_web/Fase_2_escalamiento_fail/fase1_smoke.jtl`

**Fase 3 - Sostenida:**
- Dashboard: `cloud_load_testing/escenario_1_capa_web/Fase_3_sostenido/dashboard_smoke/index.html`
- Statistics: `cloud_load_testing/escenario_1_capa_web/Fase_3_sostenido/dashboard_smoke/statistics.json`
- JTL: `cloud_load_testing/escenario_1_capa_web/Fase_3_sostenido/fase1_smoke.jtl`

**Escenario 2 - Capa Worker (Procesamiento As√≠ncrono):**

**Prueba de Workers:**
- Dashboard: `cloud_load_testing/escenario_2_capa_worker/video_10mb/dashboard_c1/index.html`
- Statistics: `cloud_load_testing/escenario_2_capa_worker/video_10mb/dashboard_c1/statistics.json`
- JTL: `cloud_load_testing/escenario_2_capa_worker/video_10mb/resultados_c1.jtl`
- Payload: `cloud_load_testing/escenario_2_capa_worker/video_10mb/mensaje_10mb.txt`

### 10.2 Configuraci√≥n JMeter

**Escenario 1 - Script de Prueba API REST:** `WebApp_Carga.jmx`
- **Sampler:** HTTP Request
- **Endpoint:** POST /api/videos/upload
- **Content-Type:** multipart/form-data
- **Payload:** Video MP4 de 18MB (file_example_MP4_1920_18MG.mp4)
- **Autenticaci√≥n:** Bearer Token JWT
- **Thread Group:** Ultimate Thread Group (rampa controlada)

**Escenario 2 - Script de Prueba Worker:** `Worker_Test.jmx`
- **Sampler:** JSR223 Sampler (Groovy)
- **Cliente:** Jedis (Redis Java Client)
- **Operaci√≥n:** LPUSH a cola "celery"
- **Payload:** Archivo de texto de 10MB (mensaje_10mb.txt)
- **Thread Group:** 10 threads durante 300 segundos
- **Variables:**
  - `REDIS_HOST`: localhost
  - `REDIS_PORT`: 6379
  - `REDIS_QUEUE`: celery
  - `PAYLOAD_FILE`: /path/to/mensaje_10mb.txt

### 10.3 Comandos √ötiles

**JMeter - Ejecuci√≥n y Reportes:**

```bash
# Ejecutar JMeter en modo no-GUI (Escenario 1)
jmeter -n -t WebApp_Carga.jmx -l results.jtl -e -o dashboard/

# Ejecutar JMeter en modo no-GUI (Escenario 2)
jmeter -n -t Worker_Test.jmx -l resultados_c1.jtl -e -o dashboard_c1/

# Generar dashboard desde JTL existente
jmeter -g results.jtl -o dashboard/
```

**AWS CloudWatch - Monitoreo de Infraestructura:**

```bash
# Monitorear CPU de instancia EC2
aws cloudwatch get-metric-statistics \
  --namespace AWS/EC2 \
  --metric-name CPUUtilization \
  --dimensions Name=InstanceId,Value=i-xxxxx \
  --start-time 2025-10-26T00:00:00Z \
  --end-time 2025-10-26T23:59:59Z \
  --period 300 \
  --statistics Average,Maximum

# Monitorear memoria disponible (requiere CloudWatch Agent)
aws cloudwatch get-metric-statistics \
  --namespace CWAgent \
  --metric-name mem_used_percent \
  --dimensions Name=InstanceId,Value=i-xxxxx \
  --start-time 2025-10-26T00:00:00Z \
  --end-time 2025-10-26T23:59:59Z \
  --period 300 \
  --statistics Average,Maximum
```

**Celery - Monitoreo de Workers (Escenario 2):**

```bash
# Ver queue depth de Redis
docker exec -it <redis_container> redis-cli LLEN celery

# Ver tareas en cola (primeras 10)
docker exec -it <redis_container> redis-cli LRANGE celery 0 10

# Ver workers activos
docker exec -it <worker_container> celery -A src.core.celery_app inspect active

# Ver stats de workers
docker exec -it <worker_container> celery -A src.core.celery_app inspect stats

# Ver tareas registradas
docker exec -it <worker_container> celery -A src.core.celery_app inspect registered

# Ver workers reservados (con tareas asignadas)
docker exec -it <worker_container> celery -A src.core.celery_app inspect reserved

# Purgar cola (CUIDADO - elimina todas las tareas)
docker exec -it <redis_container> redis-cli DEL celery
```

**Redis - Monitoreo y Debug:**

```bash
# Ver info general de Redis
docker exec -it <redis_container> redis-cli INFO

# Ver memoria usada
docker exec -it <redis_container> redis-cli INFO memory

# Ver clientes conectados
docker exec -it <redis_container> redis-cli CLIENT LIST

# Monitorear comandos en tiempo real
docker exec -it <redis_container> redis-cli MONITOR
```

**Docker - Monitoreo de Recursos:**

```bash
# Ver stats en tiempo real de todos los contenedores
docker stats

# Ver stats de contenedor espec√≠fico
docker stats <container_name>

# Ver logs de worker
docker logs -f --tail 100 <worker_container>

# Ver procesos dentro del contenedor
docker exec -it <worker_container> ps aux
```

### 10.4 Contacto

Para consultas sobre este an√°lisis:
- **Equipo:** Desarrollo de Software en la Nube
- **Repositorio:** https://github.com/sjfuentes-uniandes/desarrollo-sw-nube
- **Fecha:** 25 de Octubre, 2025

---

**Fin del Documento**


