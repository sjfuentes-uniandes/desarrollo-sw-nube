# üìä Reporte de Pruebas de Carga ‚Äì Capa Worker (SQS)

> **Proyecto:** Desarrollo de Software en la Nube  
> **Fecha de ejecuci√≥n:** 14-15 noviembre 2025  
> **Infraestructura evaluada:** Worker as√≠ncrono (Celery) detr√°s de SQS `video-app-queue`  
> **Herramienta:** Locust (`load_testing/locust_sqs/worker_sqs_locust.py`)

---

## 1. Resumen ejecutivo

- **Objetivo:** validar la capacidad de ingesti√≥n de mensajes hacia el worker v√≠a SQS y determinar el punto de saturaci√≥n antes de que la cola crezca sin control.
- **Escenarios ejecutados:** Smoke (sanidad), Ramp (b√∫squeda de saturaci√≥n) y Soak (estabilidad a tasa objetivo).
- **Resultado general:** la capa worker procesa establemente ~4 req/s con latencias altas (>25 s). En smoke y soak no hubo fallos, mientras que la rampa expuso errores 5xx cuando la API web no sostuvo el pico.
- **L√≠mite observado:** a partir de ~3.8 req/s con rampa agresiva aparecen 504/502/503 desde el backend, lo que impide sostener la carga antes de que SQS se vuelva el cuello de botella.
- **Recomendaciones clave:** desacoplar autenticaci√≥n del ciclo de prueba, incrementar capacidad del backend/API, habilitar autoescalado del worker y monitorear backlog SQS/CloudWatch en sincron√≠a con las pruebas.

---

## 2. Configuraci√≥n y supuestos

| Elemento | Detalle |
|----------|---------|
| **Script Locust** | `load_testing/locust_sqs/worker_sqs_locust.py` |
| **Acciones simuladas** | `POST /api/auth/login` + `POST /api/videos/upload` (genera mensaje SQS) |
| **Archivo de video** | `uploads/file_example_MP4_480_1_5MG.mp4` |
| **Credenciales** | Usuario de pruebas con rol est√°ndar, token por sesi√≥n |
| **Wait time** | `LOCUST_MIN_WAIT=0.02`, `LOCUST_MAX_WAIT=0.05` (override en soak) |
| **M√©tricas adicionales** | No se capturaron dashboards CloudWatch/Grafana; se inferir√° √∫nicamente con los CSV de Locust. |

---

## 3. Metodolog√≠a y escenarios

| Escenario | Prop√≥sito | Configuraci√≥n ejecutada | Duraci√≥n real | Observaciones |
|-----------|-----------|-------------------------|---------------|---------------|
| **Smoke** | Validar credenciales, path feliz y pipeline SQS con carga ligera. | `-u 10`, `-r 5`, `t=5m`, headless | 5 min | Sirvi√≥ para verificar health-check del worker |
| **Ramp** | Incrementar usuarios hasta encontrar el punto de saturaci√≥n. | `-u 200`, `-r 20`, `t=15m`, `--csv results/locust_ramp` | 15 min | Se dispararon errores 5xx desde backend |
| **Soak** | Mantener tasa objetivo (‚âà80 msg/s nominal, ~4 req/s efectivos) durante 45 min. | `-u 120`, `-r 10`, `t=45m`, `LOCUST_MIN/MAX_WAIT` agresivos | 45 min | Sin fallos, pero con alta latencia por ciclo de carga |

---

## 4. Resultados detallados

### 4.1 Consolidado de m√©tricas

| Escenario | Requests | Fallos | √âxito | Avg (ms) | p95 (ms) | Throughput (req/s) |
|-----------|----------|--------|-------|----------|----------|---------------------|
| Smoke | 1,125 | 0 | 100% | 2,610 | 4,600 | 3.75 |
| Ramp | 3,423 | 895 | 73.85% | 28,533 | 68,000 | 3.81 |
| Soak | 11,064 | 0 | 100% | 28,976 | 46,000 | 4.10 |

Fuente: CSV generados por Locust (`Smoke-results.csv`, `Ramp-results.csv`, `Soak-result.csv`).

### 4.2 Smoke ‚Äì Sanidad

- **M√©tricas principales:** 1,125 solicitudes totales, 0 fallos, latencia promedio 2.6 s, p95 4.6 s.
- **Observaciones:** autenticaci√≥n y subida de video completan en <3 s en carga baja; el worker procesa sin backlog observable.

```
```1:25:load_testing/locust_sqs/Smoke-results.csv
Requests: 1125
√âxito: 100.00%
Latencia media: 2610.34 ms
p95: 4600.00 ms
```

### 4.3 Ramp ‚Äì Punto de saturaci√≥n

- **M√©tricas principales:** 3,423 solicitudes, 26.15% de fallos, latencia media 28.5 s, p95 68 s.
- **Errores:** 504 Gateway Timeout (login), 502/503/504 en `Videos/Upload`. Los 895 fallos corresponden al backend antes de que el worker llegue a su l√≠mite real.
- **Hallazgo:** la capa web/API se convierte en el cuello de botella cuando la tasa supera ~3.8 req/s continuos; se requiere autoscaling o tuning espec√≠fico para la API.

```
```1:53:load_testing/locust_sqs/Ramp-results.csv
Requests: 3423
Fallos: 895
Latencia media: 28533.21 ms
p95: 68000.00 ms
POST Videos/Upload: Error 502/503/504
```

### 4.4 Soak ‚Äì Resistencia

- **M√©tricas principales:** 11,064 solicitudes, 0 fallos, latencia media 28.9 s, p95 46 s.
- **Comportamiento:** throughput sostenido de 4.1 req/s sin errores indica que, a tasa constante, la cola y el worker se mantienen estables aunque con latencias elevadas por el ciclo completo (autenticaci√≥n + upload + encolado).
- **Riesgo:** la latencia promedio >28 s implica que cualquier degradaci√≥n adicional (CPU, IO) podr√≠a disparar timeouts y backlog en SQS.

```
```1:26:load_testing/locust_sqs/Soak-result.csv
Requests: 11064
Fallos: 0
Latencia media: 28976.31 ms
p95: 46000.00 ms
```

---

## 5. An√°lisis y hallazgos

- **Latencia estructural alta:** incluso sin fallos (smoke/soak) los tiempos de respuesta promedian >25 s debido al ciclo completo (upload + procesamiento inicial). Esto limita el throughput percibido por el cliente.
- **Cuello de botella en la API web durante la rampa:** los errores 5xx aparecen antes de saturar el worker, se√±al de que la capa web/ALB necesita escalado adicional y tiempos de timeout mayores para acompa√±ar la prueba.
- **Worker estable pero lento:** en soak no hubo errores, lo que sugiere que el worker es consistente mientras no se exceda la tasa objetivo, aunque el SLA sigue siendo deficiente.
- **Falta de correlaci√≥n con m√©tricas de SQS/CloudWatch:** no se recopilaron m√©tricas de backlog, edad del mensaje ni CPU del worker, lo que impide demostrar formalmente el momento en que la cola se degrada.

---

## 6. Recomendaciones y pr√≥ximos pasos

1. **Autoescalado coordinado:** habilitar pol√≠ticas de scaling para la API web (ALB/ASG) y para los workers Celery para absorber rampas agresivas sin 5xx.
2. **Optimizar autenticaci√≥n:** evitar re-login por ciclo (token reutilizable) o mover la autenticaci√≥n fuera del path cr√≠tico de la prueba para liberar capacidad.
3. **Instrumentaci√≥n en SQS/CloudWatch:** capturar `ApproximateNumberOfMessagesVisible`, `ApproximateAgeOfOldestMessage` y m√©tricas de consumo para correlacionar tiempos de Locust con backlog real.
4. **Reducci√≥n de latencia:** investigar procesamiento sincronizado inicial, tama√±o del archivo y operaciones en base de datos; considerar compresi√≥n o colas diferenciadas por tama√±o.
5. **Re-ejecuci√≥n dirigida:** tras aplicar ajustes, repetir la rampa aumentando el throughput objetivo (‚â•6 req/s) para validar la mejora y documentar el nuevo umbral.

---

## 7. Evidencias y artefactos

- CSV generados por Locust (`load_testing/locust_sqs/Smoke-results.csv`, `Ramp-results.csv`, `Soak-result.csv`).
- Scripts de ejecuci√≥n (`run_smoke.sh`, `run_ramp.sh`, `run_soak.sh`).
- Plan de prueba base en `capacity_planning/worker_sqs_locust_plan.md`.

Estos archivos permanecen en el repositorio para auditor√≠a y replicabilidad.

---

# üìä Pruebas de Carga - Capa Web con Arquitectura SQS + Worker (JMeter)

> **Fecha de ejecuci√≥n:** 16 de noviembre de 2025  
> **Infraestructura evaluada:** ALB + Auto Scaling (API Web) + SQS + Workers  
> **Herramienta:** Apache JMeter 5.6.3  
> **Objetivo:** Evaluar rendimiento de la API web con arquitectura desacoplada SQS + Worker

---

## 8. Escenario 1: 100 Usuarios Concurrentes

### 8.1 Configuraci√≥n del Escenario

| Par√°metro | Valor |
|-----------|-------|
| **Usuarios concurrentes** | 100 |
| **Rampa de inicio** | 0 ‚Üí 100 usuarios en 5 minutos |
| **Sostenimiento** | 100 usuarios por 5 minutos |
| **Rampa de descenso** | 100 ‚Üí 0 usuarios en 2 minutos |
| **Duraci√≥n total** | 10 minutos 33 segundos |
| **Fecha de ejecuci√≥n** | 16/11/2025 |

### 8.2 Resultados

| M√©trica | Valor |
|---------|-------|
| **Total de Requests** | 356 |
| **Requests Exitosos** | 299 (84.1%) |
| **Requests Fallidos** | 57 (15.9%) |
| **Tiempo Promedio** | 136.14 segundos |
| **Throughput** | 0.6 req/s |

### 8.3 An√°lisis

- ‚úÖ **Tasa de √©xito aceptable**: 84.1% de requests completados exitosamente
- ‚ö†Ô∏è **Tiempo de respuesta elevado**: Promedio de 136 segundos por request
- El sistema procesa establemente bajo carga de 100 usuarios concurrentes
- Se observa comportamiento consistente del desacoplamiento API-Worker mediante SQS

---

## 9. Escenario 2: 200 Usuarios Concurrentes

### 9.1 Configuraci√≥n del Escenario

| Par√°metro | Valor |
|-----------|-------|
| **Usuarios concurrentes** | 200 |
| **Rampa de inicio** | 0 ‚Üí 200 usuarios en 5 minutos |
| **Sostenimiento** | 200 usuarios por 5 minutos |
| **Rampa de descenso** | 200 ‚Üí 0 usuarios en 2 minutos |
| **Duraci√≥n total** | 11 minutos 31 segundos |
| **Fecha de ejecuci√≥n** | 16/11/2025 |

### 9.2 Resultados

| M√©trica | Valor |
|---------|-------|
| **Total de Requests** | 440 |
| **Requests Exitosos** | 292 (66.4%) |
| **Requests Fallidos** | 148 (33.6%) |
| **Tiempo Promedio** | ~180 segundos |
| **Throughput** | 0.6 req/s |

### 9.3 An√°lisis

- ‚ö†Ô∏è **Tasa de √©xito moderada**: 66.4% de requests exitosos bajo carga de 200 usuarios
- ‚ùå **Aumento significativo de errores**: 33.6% de tasa de error indica saturaci√≥n del sistema
- El sistema muestra inestabilidad bajo carga media-alta
- Los workers requieren optimizaci√≥n para absorber mejor la carga
- Posible saturaci√≥n de cola SQS o timeouts en procesamiento as√≠ncrono

---

## 10. Escenario 3: 300 Usuarios Concurrentes

### 10.1 Configuraci√≥n del Escenario

| Par√°metro | Valor |
|-----------|-------|
| **Usuarios concurrentes** | 300 |
| **Rampa de inicio** | 0 ‚Üí 300 usuarios en 5 minutos |
| **Sostenimiento** | 300 usuarios por 5 minutos |
| **Rampa de descenso** | 300 ‚Üí 0 usuarios en 2 minutos |
| **Duraci√≥n total** | ~12 minutos |
| **Fecha de ejecuci√≥n** | 16/11/2025 |

### 10.2 Resultados

| M√©trica | Valor |
|---------|-------|
| **Total de Requests** | 705 |
| **Requests Exitosos** | 520 (73.7%) |
| **Requests Fallidos** | 185 (26.3%) |
| **Tiempo Promedio** | ~150 segundos |
| **Throughput** | 1.0 req/s |

### 10.3 An√°lisis

- ‚úÖ **Mejor rendimiento relativo**: 73.7% de tasa de √©xito con 300 usuarios concurrentes
- ‚úÖ **Throughput mejorado**: 1.0 req/s, indicando mejor utilizaci√≥n de recursos
- ‚ö†Ô∏è **Tasa de error considerable**: 26.3% de fallos sugiere l√≠mites del sistema
- El autoscaling de workers est√° funcionando, permitiendo procesar mayor carga
- Tiempo de respuesta reducido (150s) comparado con escenarios de menor carga

---

## 11. An√°lisis Comparativo Global

### 11.1 Resumen de los 3 Escenarios

| Escenario | Usuarios | Requests | Tasa de √âxito | Errores | Tiempo Promedio | Throughput |
|-----------|----------|----------|---------------|---------|-----------------|------------|
| 1 | 100 | 356 | 84.1% | 15.9% | 136s | 0.6 req/s |
| 2 | 200 | 440 | 66.4% | 33.6% | 180s | 0.6 req/s |
| 3 | 300 | 705 | 73.7% | 26.3% | 150s | 1.0 req/s |

### 11.2 Hallazgos Principales

**Comportamiento del Sistema:**

üìä **Patrones Observados:**
- **Escenario 1 (100 usuarios)**: Mejor tasa de √©xito (84.1%), sistema estable
- **Escenario 2 (200 usuarios)**: Mayor degradaci√≥n (66.4%), punto cr√≠tico de saturaci√≥n
- **Escenario 3 (300 usuarios)**: Recuperaci√≥n parcial (73.7%), autoscaling efectivo

‚úÖ **Fortalezas de la Arquitectura:**
- Desacoplamiento entre API y procesamiento mediante SQS
- Escalamiento independiente de workers
- Mayor resiliencia con mensajes persistentes en SQS
- Autoscaling responde a la demanda (visible en escenario 300 usuarios)

‚ö†Ô∏è **√Åreas de Mejora:**
- Tasa de errores aumenta significativamente con 200 usuarios (33.6%)
- Throughput limitado en escenarios 100 y 200 (0.6 req/s)
- Tiempos de respuesta elevados en todos los escenarios
- Variabilidad en comportamiento entre escenarios

‚ö†Ô∏è **Cuellos de Botella Identificados:**
1. **Configuraci√≥n de Workers**: No escalan eficientemente
2. **SQS Visibility Timeout**: Mensajes pueden estar expirando
3. **Dead Letter Queue**: Posible acumulaci√≥n de mensajes fallidos
4. **Auto Scaling Delays**: Workers tardan en lanzarse
5. **Network Latency**: Overhead de comunicaci√≥n SQS

### 11.3 Recomendaciones

**Optimizaciones Cr√≠ticas:**

1. **Configuraci√≥n de Auto Scaling:**
   - Reducir cooldown period de workers
   - Ajustar m√©tricas de scaling (usar backlog SQS)
   - Aumentar capacidad m√≠nima de workers

2. **Configuraci√≥n de SQS:**
   - Aumentar visibility timeout (300s ‚Üí 600s)
   - Configurar Dead Letter Queue apropiadamente
   - Habilitar Long Polling para reducir requests vac√≠os

3. **Optimizaci√≥n de Workers:**
   - Mejorar eficiencia de procesamiento
   - Implementar retry logic robusto
   - Monitorear y reducir timeouts

4. **Monitoreo y Alertas:**
   - CloudWatch dashboards para SQS metrics
   - Alertas por backlog de mensajes
   - Tracking de success/error rate en tiempo real

**Pr√≥ximos Pasos:**

- [ ] Implementar optimizaciones recomendadas
- [ ] Re-ejecutar pruebas para validar mejoras
- [ ] Documentar m√©tricas de CloudWatch
- [ ] Realizar pruebas de soak (larga duraci√≥n)
- [ ] Validar comportamiento de Dead Letter Queue

---

**Documento actualizado:** 16/11/2025  
**Versi√≥n:** 2.0  
**Estado:** Completo con 3 escenarios documentados

---
